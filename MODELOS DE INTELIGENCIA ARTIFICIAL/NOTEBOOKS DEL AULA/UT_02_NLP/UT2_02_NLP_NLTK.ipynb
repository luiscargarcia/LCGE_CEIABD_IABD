{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "749d8919",
   "metadata": {},
   "source": [
    "# NLP con librería NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10e4d3c",
   "metadata": {},
   "source": [
    "En la instalación de **NLTK** no se incluyen los recursos necesarios para trabajar en PLN (las reglas de puntuación o las stopwords).    \n",
    "Por tanto, la primera vez que se ejecuten las funciones de librería se solicitará que se descarguen estos recursos.    \n",
    "Esto es algo que se puede hacer simplemente indicando a la función ```download()``` de *NLTK* los recursos requeridos.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3474fc28d17e9201",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Análisis de sentimientos en inglés con NLTK.   \n",
    "\n",
    "Para realizar análisis de sentimientos con NLTK es necesario importar lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cb03d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos librerías\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T13:45:32.776938Z",
     "start_time": "2024-03-11T13:45:32.771958Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('popular')\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bd70ba",
   "metadata": {},
   "source": [
    "### 1. Preprocesamiento de datos\n",
    "\n",
    "Antes de poder realizar el análisis de sentimientos con NLTK, es necesario preprocesar los mensajes de texto para normalizar. \n",
    "\n",
    "Los pasos a llevar a cabo son:\n",
    "\n",
    "- **Tokenización**: dividir el texto en palabras o frases más pequeñas llamadas tokens.\n",
    "- **Eliminación de signos de puntuación y caracteres especiales**.\n",
    "- **Conversión de texto a minúsculas** para normalizar el texto.\n",
    "- **Eliminación de las stopwords** o palabras irrelevantes para el mensaje tales como “a”, “el”, “y”, etc.\n",
    "- **Reducción de las palabras a su forma base (lemas)**.   \n",
    "\n",
    "Estos pasos se pueden implementar con funciones de NLTK, tal como se muestra en el siguiente ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e0cb0f7fb5e2df0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T13:45:42.101426Z",
     "start_time": "2024-03-11T13:45:40.835424Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love content artificial intelligence subject notebook fantastic'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Procesado de texto\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "text = \"I love the content on the Artificial Intelligence subjects, notebooks are fantastic.\"\n",
    "\n",
    "# Tokenización\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Eliminación de signos de puntuación\n",
    "tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "# Conversión a minúsculas\n",
    "tokens = [token.lower() for token in tokens]\n",
    "\n",
    "# Eliminación de stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "# Lematización\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Reconstrucción del texto preprocesado\n",
    "preprocessed_text = ' '.join(tokens)\n",
    "preprocessed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2607f2b3",
   "metadata": {},
   "source": [
    "Proceso ejecutado:    \n",
    "\n",
    "- En primer lugar, se debe tokenizar las frase mediante la función ```word_tokenize()```, lo que divide está en una lista de palabras y signos de puntuación.    \n",
    "- Posteriormente, mediante con herramientas estándar de Python, se eliminan los tokens que estén en la lista de signos de puntuación (```string.punctuation```) y se convierten los todo el texto a minúsculas.    \n",
    "- Una vez homogeneizado el texto, se eliminan las stopwords que incluye NLTK.    \n",
    "- A la hora de importar las stopwords es necesario indicar el idioma con el que se está trabajando ya que estas son diferentes. \n",
    "- Finalmente se lematiza los tokens para eliminar plurales y derivaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fdd3a9",
   "metadata": {},
   "source": [
    "### 2. Extracción de características   \n",
    "\n",
    "Una vez preprocesado el texto, es necesario extraer las características de este antes de poder entrenar un modelo.    \n",
    "Lo más habitual es emplear la frecuencia de las palabras como características. En NLTK esto se implementa mediante la clase **FreqDist** y se muestra en el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e721298729c09180",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T13:45:43.870604Z",
     "start_time": "2024-03-11T13:45:43.863419Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 1,\n",
       " 'content': 1,\n",
       " 'artificial': 1,\n",
       " 'intelligence': 1,\n",
       " 'subject': 1,\n",
       " 'notebook': 1,\n",
       " 'fantastic': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "features = {}\n",
    "words = word_tokenize(preprocessed_text)\n",
    "word_freq = FreqDist(words)\n",
    "\n",
    "for word, freq in word_freq.items():\n",
    "    features[word] = freq\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf40f9c5",
   "metadata": {},
   "source": [
    "Obtenemos como resultado un diccionario con la palabra clave y el valor es el número de ocurrencias de cada una de estas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563f2051",
   "metadata": {},
   "source": [
    "## 3. Conjunto de datos de entrenamiento   \n",
    "\n",
    "Para entrenar un modelo es necesario contar con un conjunto de datos de entrenamiento.   \n",
    "A tal efecto, se crea una lista de tuplas con el mensaje y la etiqueta que se desea que le corresponda para el entrenamiento.    \n",
    "\n",
    "A modo de ejemplo se puede probar con un listado de siete mensajes similar al siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20063f68b2a1481d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T13:50:11.954630Z",
     "start_time": "2024-03-11T13:50:11.948613Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    (\"I love the content on the Artificial Intelligence subjects, notebooks are fantastic.\", \"positive\"),\n",
    "    (\"The code does not work, it gave me an error when executing it.\", \"negative\"),\n",
    "    (\"I love this product!\", \"positive\"),\n",
    "    (\"This movie was terrible.\", \"negative\"),\n",
    "    (\"The weather is nice today.\", \"positive\"),\n",
    "    (\"I feel so sad about the news.\", \"negative\"),\n",
    "    (\"It's just an average book.\", \"neutral\"),\n",
    "    (\"I don´t like milk.\", \"negative\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297e1205",
   "metadata": {},
   "source": [
    "### Factorización del código\n",
    "\n",
    "Se puede factorizar el código anterior para facilitar su uso a la hora de entrenar un modelo y que, globalmente, resulte más modular.    \n",
    "\n",
    "Para ello se pueden crear dos funciones:    \n",
    "- ```preprocess_text()``` para el preprocesado de texto y \n",
    "- ```extract_features()``` para la extracción de características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d59c5cad495e7ff4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T13:50:13.178009Z",
     "start_time": "2024-03-11T13:50:13.171944Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Realiza el preprocesamiento básico de un texto en inglés utilizando NLTK.\n",
    "\n",
    "    Args:\n",
    "        text (str): El texto a ser preprocesado.\n",
    "\n",
    "    Returns:\n",
    "        str: El texto preprocesado.\n",
    "    \"\"\"\n",
    "    # Tokenización\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Eliminación de signos de puntuación\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    # Conversión a minúsculas\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # Eliminación de stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lematización\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Reconstrucción del texto preprocesado\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "\n",
    "    return preprocessed_text\n",
    "\n",
    "\n",
    "def extract_features(text):\n",
    "    \"\"\"\n",
    "    Extrae las características del texto utilizando NLTK y devuelve un diccionario de características.\n",
    "\n",
    "    Args:\n",
    "        text (str): El texto del cual extraer características.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un diccionario que representa las características extraídas del texto.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    words = word_tokenize(text)\n",
    "    word_freq = FreqDist(words)\n",
    "\n",
    "    for word, freq in word_freq.items():\n",
    "        features[word] = freq\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4069a3",
   "metadata": {},
   "source": [
    "## 4. Entrenamiento del modelo.   \n",
    "\n",
    "El análisis de sentimientos con NLTK se puede realizar usando un clasificador basado en Naive Bayes.    \n",
    "NLTK proporciona una clase en la que se implementa este tipo de clasificadores.     \n",
    "\n",
    "Empleando esta clase y las funciones creadas en la sección anterior se puede entrenar un modelo con los datos de ejemplo, tal como se muestra a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "378aafc2d7aed633",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T13:50:16.361382Z",
     "start_time": "2024-03-11T13:50:16.350525Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "# Preprocesamiento de los datos de entrenamiento\n",
    "preprocessed_training_data = [(preprocess_text(text), label) for text, label in training_data]\n",
    "\n",
    "# Extracción de características de los datos de entrenamiento\n",
    "training_features = [(extract_features(text), label) for text, label in preprocessed_training_data]\n",
    "\n",
    "# Entrenamiento del clasificador Naive Bayes\n",
    "classifier = NaiveBayesClassifier.train(training_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f6d38f",
   "metadata": {},
   "source": [
    "## 5. Clasificación de nuevos textos.\n",
    "\n",
    "Una vez que el modelo está entrando, este se puede usar para clasificar nuevos textos.    \n",
    "Simplemente es necesario preprocesar y extraer las características de la nueva cadena de texto para realizar la predicción con el clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a7ef191ae36faec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T13:51:43.057688Z",
     "start_time": "2024-03-11T13:51:43.050866Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "# Nuevo texto para clasificar\n",
    "# new_text = \"I really enjoy the concert\" # positivo\n",
    "# new_text = \"The concert was terrible\"     # negativo\n",
    "new_text = \"I really love the concert\"    # negativo\n",
    "\n",
    "# Preprocesamiento del nuevo texto\n",
    "preprocessed_text = preprocess_text(new_text)\n",
    "\n",
    "# Extracción de características del nuevo texto\n",
    "features = extract_features(preprocessed_text)\n",
    "\n",
    "# Clasificación del nuevo texto\n",
    "sentiment = classifier.classify(features)\n",
    "print(\"Sentiment:\", sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d0e98b859b0d14",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Análisis de sentimientos en español  \n",
    "\n",
    "En este noteboook se ha explicado cómo hacer análisis de sentimiento en inglés. Si se usa el ejemplo para trabajar con texto en español, u otros idiomas, el resultado no será satisfactorio dado que se ha usado el listado de stopwords del inglés y un lematizador (*WordNetLemmatizer*) que no es adecuado para el español.\n",
    "\n",
    "Por eso, para realizar análisis de sentimientos en español se dispone de otro notebook que usa la librería **spaCy**, la cúal dispone de las herramientas adecuadas para llevar a cabo correctamente esta tarea.   \n",
    "\n",
    "##  Conclusiones.\n",
    "NLTK es la librería de referencia para el procesado del lenguaje natural (PLN). Se trata de una librería que facilita el trabajo cuando se desea realizar análisis de sentimientos.    \n",
    "Aunque, como ya hemos dicho, funciones clave como la lematización solamente funcionan en inglés, el uso de NLTK facilita comprender los pasos necesarios para realizar este tipo de análisis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bfd4cf",
   "metadata": {},
   "source": [
    "### Anexo. Extracción de información y visualización de análisis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1ae1c5",
   "metadata": {},
   "source": [
    "Importamos librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6e955e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package english_wordnet to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/english_wordnet.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets_json.zip.\n",
      "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db6aea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787eef14",
   "metadata": {},
   "source": [
    "Sentencia a analizar (en inglés):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4025598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77208b66",
   "metadata": {},
   "source": [
    "A continuación, aplicamos a la sentencia la tokenización de palabras y el etiquetado de partes del discurso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f1b1b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenización - original\n",
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent, lang='eng')\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fd9a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenización - con perceptrón (opción más rápida y cargando menos datos)\n",
    "from nltk.tag import PerceptronTagger\n",
    "tagger = PerceptronTagger()\n",
    "\n",
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = tagger.tag(sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a8405b",
   "metadata": {},
   "source": [
    "Veamos lo que vamos a obtener:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db0b98d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('European', 'JJ'),\n",
       " ('authorities', 'NNS'),\n",
       " ('fined', 'VBD'),\n",
       " ('Google', 'NNP'),\n",
       " ('a', 'DT'),\n",
       " ('record', 'NN'),\n",
       " ('$', '$'),\n",
       " ('5.1', 'CD'),\n",
       " ('billion', 'CD'),\n",
       " ('on', 'IN'),\n",
       " ('Wednesday', 'NNP'),\n",
       " ('for', 'IN'),\n",
       " ('abusing', 'VBG'),\n",
       " ('its', 'PRP$'),\n",
       " ('power', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('mobile', 'JJ'),\n",
       " ('phone', 'NN'),\n",
       " ('market', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('ordered', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('company', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('alter', 'VB'),\n",
       " ('its', 'PRP$'),\n",
       " ('practices', 'NNS')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = preprocess(ex)\n",
    "sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5291d8df",
   "metadata": {},
   "source": [
    "Obtenemos una lista de tuplas que contiene las palabras individuales de la frase y su parte de habla asociada.\n",
    "\n",
    "Ahora implementaremos el troceado de frases nominales para identificar entidades con nombre utilizando una expresión regular que consiste en reglas que indican cómo deben trocearse las frases.\n",
    "\n",
    "Nuestro patrón de troceado consiste en una regla, según la cual debe formarse una frase nominal, NP, siempre que el troceador encuentre un determinante opcional, DT, seguido de cualquier número de adjetivos, JJ, y después un sustantivo, NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a289ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = 'NP: {<DT>?<JJ>*<NN>}' # NP: Noun Phrase (sustantivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc886895",
   "metadata": {},
   "source": [
    "### CHUNKING (Análisis sintáctico)   \n",
    "\n",
    "Utilizando este patrón, creamos un analizador sintáctico por trozos y lo probamos con nuestra frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa78bea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  European/JJ\n",
      "  authorities/NNS\n",
      "  fined/VBD\n",
      "  Google/NNP\n",
      "  (NP a/DT record/NN)\n",
      "  $/$\n",
      "  5.1/CD\n",
      "  billion/CD\n",
      "  on/IN\n",
      "  Wednesday/NNP\n",
      "  for/IN\n",
      "  abusing/VBG\n",
      "  its/PRP$\n",
      "  (NP power/NN)\n",
      "  in/IN\n",
      "  (NP the/DT mobile/JJ phone/NN)\n",
      "  (NP market/NN)\n",
      "  and/CC\n",
      "  ordered/VBD\n",
      "  (NP the/DT company/NN)\n",
      "  to/TO\n",
      "  alter/VB\n",
      "  its/PRP$\n",
      "  practices/NNS)\n"
     ]
    }
   ],
   "source": [
    "cp = nltk.RegexpParser(pattern)\n",
    "cs = cp.parse(sent)\n",
    "print(cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad64ff8",
   "metadata": {},
   "source": [
    "El resultado puede leerse como un árbol o una jerarquía con S como primer nivel, que denota la frase. también podemos visualizarlo gráficamente, usando el siguiente código:     \n",
    "\n",
    "Nota: necesario instalar tkinter en terminal (apt-get install python3-tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b486086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.draw.tree import TreeView\n",
    "\n",
    "# Guarda el árbol como una imagen\n",
    "TreeView(cs)._cframe.print_to_file('tree.ps')\n",
    "\n",
    "# Mostrar el archivo en el notebook\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "img = Image.open('tree.ps')\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1bcde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f5fd0f",
   "metadata": {},
   "source": [
    "**Etiquetas IOB**   \n",
    "\n",
    "Las etiquetas IOB (Inside-Outside-Beginning) son un formato estándar para etiquetar secuencias de texto en tareas de procesamiento de lenguaje natural (NLP), especialmente en el reconocimiento de entidades nombradas (NER) y análisis sintáctico.\n",
    "\n",
    "*Significado de las etiquetas IOB:*\n",
    "- B = Beginning → Indica el comienzo de una entidad nombrada.\n",
    "- I = Inside → Indica que la palabra es parte de la misma entidad nombrada que la palabra anterior.\n",
    "- O = Outside → Indica que la palabra no forma parte de ninguna entidad nombrada.   \n",
    "\n",
    "Tipos de entidades comunes en NER:\n",
    "- PERSON → Persona\n",
    "- ORG → Organización\n",
    "- LOC → Ubicación física\n",
    "- GPE → Entidad geopolítica (país, ciudad, etc.)\n",
    "- DATE → Fecha\n",
    "- MONEY → Cantidad de dinero   \n",
    "\n",
    "Algunas etiquetas de posición (POS) comunes:   \n",
    "\n",
    "| Etiqueta POS | Significado | Ejemplo |\n",
    "|--------------|-------------|---------|\n",
    "|NN|\tSustantivo singular|\tdog, car, house|\n",
    "|NNS|\tSustantivo plural|\tdogs, cars, houses|\n",
    "|NNP|\tSustantivo propio singular|\tJohn, London|\n",
    "|NNPS|\tSustantivo propio plural|\tAmericans, Germans|\n",
    "|VB|\tVerbo base|\tbe, have, do|\n",
    "|VBD|\tVerbo en pasado|\twas, had, did|\n",
    "|VBG|\tVerbo en gerundio|\tbeing, having, doing|\n",
    "|VBN|\tVerbo en participio pasado|\tbeen, had, done|\n",
    "|VBP|\tVerbo en presente (excepto tercera persona)|\tam, have, do|\n",
    "|VBZ|\tVerbo en presente (tercera persona)|\tis, has, does|\n",
    "|JJ|\tAdjetivo|\tbig, good, blue|\n",
    "|JJR|\tAdjetivo comparativo|\tbigger, better|\n",
    "|JJS|\tAdjetivo superlativo|\tbiggest, best|\n",
    "|RB|\tAdverbio|\tquickly, silently|\n",
    "|RBR|\tAdverbio comparativo|\tfaster, better|\n",
    "|RBS|\tAdverbio superlativo|\tfastest, best|\n",
    "|PRP|\tPronombre personal|\tI, you, he, she|\n",
    "|PRP$|\tPronombre posesivo|\tmy, your, his|\n",
    "|DT|\tDeterminante|\tthe, a, an|\n",
    "|IN|\tPreposición o conjunción subordinante|\tin, on, that|\n",
    "|CC|\tConjunción de coordinación|\tand, but, or|\n",
    "|CD|\tNúmero cardinal|\tone, two, 100|\n",
    "|EX|\tExistencial \"there\"|\tthere|\n",
    "|FW|\tPalabra extranjera|\tc’est, je ne sais quoi|\n",
    "|LS|\tSímbolo de lista|\tA), B), 1.|\n",
    "|MD|\tVerbo modal\t| can, must |\n",
    "|PDT|\tPredeterminante\t| all, both, half |\n",
    "|POS|\tMarca de posesión|\t's |\n",
    "|SYM|\tSímbolo|\t@, #, $ |\n",
    "|TO|\t\"to\" como preposición o infinitivo|\tto |\n",
    "|UH|\tInterjección |\toh, oops, wow |\n",
    "|WDT|\tPronombre relativo|\twhich, that|\n",
    "|WP|\tPronombre wh|\twho, what |\n",
    "|WP$|\tPronombre posesivo wh |\twhose|\n",
    "|WRB|\tAdverbio wh |\twhere, when |\n",
    "\n",
    "En el siguiente bloque de código hacemos uso de las etiquetas IOB, que se han convertido en la forma estándar de representar estructuras de trozos en los archivos, y en este ejemplo también utilizaremos dicho formato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ff64148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('European', 'JJ', 'O'),\n",
      " ('authorities', 'NNS', 'O'),\n",
      " ('fined', 'VBD', 'O'),\n",
      " ('Google', 'NNP', 'O'),\n",
      " ('a', 'DT', 'B-NP'),\n",
      " ('record', 'NN', 'I-NP'),\n",
      " ('$', '$', 'O'),\n",
      " ('5.1', 'CD', 'O'),\n",
      " ('billion', 'CD', 'O'),\n",
      " ('on', 'IN', 'O'),\n",
      " ('Wednesday', 'NNP', 'O'),\n",
      " ('for', 'IN', 'O'),\n",
      " ('abusing', 'VBG', 'O'),\n",
      " ('its', 'PRP$', 'O'),\n",
      " ('power', 'NN', 'B-NP'),\n",
      " ('in', 'IN', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('mobile', 'JJ', 'I-NP'),\n",
      " ('phone', 'NN', 'I-NP'),\n",
      " ('market', 'NN', 'B-NP'),\n",
      " ('and', 'CC', 'O'),\n",
      " ('ordered', 'VBD', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('company', 'NN', 'I-NP'),\n",
      " ('to', 'TO', 'O'),\n",
      " ('alter', 'VB', 'O'),\n",
      " ('its', 'PRP$', 'O'),\n",
      " ('practices', 'NNS', 'O')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from pprint import pprint\n",
    "iob_tagged = tree2conlltags(cs)\n",
    "pprint(iob_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b15609",
   "metadata": {},
   "source": [
    "En esta representación, hay un token por línea, cada uno con su etiqueta de parte de palabra y su etiqueta de entidad con nombre.    \n",
    "\n",
    "A partir de este corpus de entrenamiento, podemos construir un etiquetador que sirva para etiquetar nuevas frases; y utilizar la función `nltk.chunk.conlltags2tree()` para convertir las secuencias de etiquetas en un árbol de trozos.   \n",
    "\n",
    "Con la función `nltk.ne_chunk()`, podemos reconocer entidades con nombre utilizando un clasificador. El clasificador añade etiquetas de categoría como PERSONA, ORGANIZACIÓN y GPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db73f540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE European/JJ)\n",
      "  authorities/NNS\n",
      "  fined/VBD\n",
      "  (PERSON Google/NNP)\n",
      "  a/DT\n",
      "  record/NN\n",
      "  $/$\n",
      "  5.1/CD\n",
      "  billion/CD\n",
      "  on/IN\n",
      "  Wednesday/NNP\n",
      "  for/IN\n",
      "  abusing/VBG\n",
      "  its/PRP$\n",
      "  power/NN\n",
      "  in/IN\n",
      "  the/DT\n",
      "  mobile/JJ\n",
      "  phone/NN\n",
      "  market/NN\n",
      "  and/CC\n",
      "  ordered/VBD\n",
      "  the/DT\n",
      "  company/NN\n",
      "  to/TO\n",
      "  alter/VB\n",
      "  its/PRP$\n",
      "  practices/NNS)\n"
     ]
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "# Tokenizar, etiquetar y hacer el análisis de entidades nombradas\n",
    "tokens = word_tokenize(ex)\n",
    "pos_tags = pos_tag(tokens)\n",
    "ne_tree = ne_chunk(pos_tags)\n",
    "print(ne_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83251eb",
   "metadata": {},
   "source": [
    "Podemos comprobar que, lamentablemente, se identifica a GOOGLE como una entidad de tipo PERSON (Persona) 😂"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b113e12c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

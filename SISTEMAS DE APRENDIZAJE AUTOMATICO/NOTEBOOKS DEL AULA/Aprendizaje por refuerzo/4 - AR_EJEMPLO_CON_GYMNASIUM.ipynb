{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje por Refuerzo con **Gymnasium**: Guía práctica    \n",
    "\n",
    "\n",
    "Gymnasium es una biblioteca Python de código abierto diseñada para apoyar el desarrollo de algoritmos de RL. Para facilitar la investigación y el desarrollo en VR, Gymnasium proporciona: \n",
    "\n",
    "- Una amplia variedad de entornos, desde juegos sencillos hasta problemas que imitan escenarios de la vida real.\n",
    "- API racionalizadas y envoltorios para interactuar con los entornos.\n",
    "- La posibilidad de crear entornos personalizados y aprovechar el marco de la API.   \n",
    "\n",
    "Los desarrolladores pueden construir algoritmos de RL y utilizar llamadas a la API para tareas como:\n",
    "\n",
    "- Pasar al entorno la acción elegida por el agente.\n",
    "- Conocer el estado del entorno y la recompensa tras cada acción. \n",
    "- Entrenar el modelo.\n",
    "- Comprobación del funcionamiento del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelado con ***Gymnasium***   \n",
    "\n",
    "Gymnasium necesita versiones específicas (no las últimas versiones) de varios programas dependientes como NumPy y PyTorch.    \n",
    "Por tanto, puede ser recomendable crear un nuevo entorno Conda o venv o un nuevo contenedor para instalar, utilizar Gymnasium y ejecutar los programas de RL.    \n",
    "\n",
    "Para instalar Gymnasium en un servidor o máquina local, se debe ejecutar:    \n",
    "    \n",
    "`pip install gymnasium`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorar entornos proporcionados por ***Gymnasium***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CartPole-v0\n",
      "CartPole-v1\n",
      "MountainCar-v0\n",
      "MountainCarContinuous-v0\n",
      "Pendulum-v1\n",
      "Acrobot-v1\n",
      "phys2d/CartPole-v0\n",
      "phys2d/CartPole-v1\n",
      "phys2d/Pendulum-v0\n",
      "LunarLander-v3\n",
      "LunarLanderContinuous-v3\n",
      "BipedalWalker-v3\n",
      "BipedalWalkerHardcore-v3\n",
      "CarRacing-v3\n",
      "Blackjack-v1\n",
      "FrozenLake-v1\n",
      "FrozenLake8x8-v1\n",
      "CliffWalking-v0\n",
      "Taxi-v3\n",
      "tabular/Blackjack-v0\n",
      "tabular/CliffWalking-v0\n",
      "Reacher-v2\n",
      "Reacher-v4\n",
      "Reacher-v5\n",
      "Pusher-v2\n",
      "Pusher-v4\n",
      "Pusher-v5\n",
      "InvertedPendulum-v2\n",
      "InvertedPendulum-v4\n",
      "InvertedPendulum-v5\n",
      "InvertedDoublePendulum-v2\n",
      "InvertedDoublePendulum-v4\n",
      "InvertedDoublePendulum-v5\n",
      "HalfCheetah-v2\n",
      "HalfCheetah-v3\n",
      "HalfCheetah-v4\n",
      "HalfCheetah-v5\n",
      "Hopper-v2\n",
      "Hopper-v3\n",
      "Hopper-v4\n",
      "Hopper-v5\n",
      "Swimmer-v2\n",
      "Swimmer-v3\n",
      "Swimmer-v4\n",
      "Swimmer-v5\n",
      "Walker2d-v2\n",
      "Walker2d-v3\n",
      "Walker2d-v4\n",
      "Walker2d-v5\n",
      "Ant-v2\n",
      "Ant-v3\n",
      "Ant-v4\n",
      "Ant-v5\n",
      "Humanoid-v2\n",
      "Humanoid-v3\n",
      "Humanoid-v4\n",
      "Humanoid-v5\n",
      "HumanoidStandup-v2\n",
      "HumanoidStandup-v4\n",
      "HumanoidStandup-v5\n",
      "GymV21Environment-v0\n",
      "GymV26Environment-v0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "for i in gym.envs.registry.keys():\n",
    "\tprint(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También se puede visitar la página *[Gymnasium](https://gymnasium.farama.org/)*. La columna de la izquierda dispone de enlaces a todos los entornos. La página web de cada entorno incluye detalles sobre él, como acciones, estados, etc. \n",
    "\n",
    "Los entornos están organizadosen categorías como Control Clásico, Box2D y más. A continuación, se enumeran algunos de los entornos comunes de cada grupo:\n",
    "\n",
    "- Control clásico: Estos son los entornos canónicos utilizados en el desarrollo de la VR; son la base de muchos ejemplos de libros de texto. Proporcionan la mezcla adecuada de complejidad y sencillez para probar y comparar nuevos algoritmos de RL. Los entornos de control clásicos Gymnasium incluyen \n",
    "    - Acrobot\n",
    "    - Poste de carro\n",
    "    - Coche de montaña Discreto\n",
    "    - Coche de montaña Continuo\n",
    "    - Péndulo\n",
    "- Box2D: Box2D es un motor de física 2D para juegos. Los entornos basados en este motor incluyen juegos sencillos como:\n",
    "    - Lunar Lander\n",
    "    - Carreras de coches\n",
    "- ToyText: Son entornos pequeños y sencillos que suelen utilizarse para depurar algoritmos de RL. Muchos de estos entornos se basan en el modelo de mundo en cuadrícula pequeña y en juegos de cartas sencillos. Algunos ejemplos son: \n",
    "    - Blackjack\n",
    "    - Taxi\n",
    "    - Lago Helado\n",
    "- MuJoCo: La dinámica multiarticular con contacto (MuJoCo) es un motor de física de código abierto que simula entornos para aplicaciones como robótica, biomecánica, ML, etc. Los entornos MuJoCo en Gymnasium incluyen:\n",
    "    - Hormiga\n",
    "    - Hopper\n",
    "    - Humanoid\n",
    "    - Nadador\n",
    "    - ...   \n",
    "\n",
    "Además de los entornos incorporados, Gymnasium puede utilizarse con muchos entornos externos utilizando la misma API. \n",
    "\n",
    "En este notebook utilizaremos uno de los entornos canónicos de Control Clásico.    \n",
    "Para importar un entorno concreto, se utiliza el comando `.make()` y se pasa el nombre del entorno como argumento. Por ejemplo, para crear un nuevo entorno basado en *CartPole (versión 1)*, se utiliza el comando que aparece a continuación: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entendiendo los conceptos del aprendizaje por refuerzo en *Gymnasium*   \n",
    "\n",
    "Como ya se ha comentado, el Aprendizaje por Refuerzo consiste en un agente (robot o similar) que interactúa con su entorno.    \n",
    "Una política decide las acciones del agente.    \n",
    "En función de las acciones del agente, el entorno le da una recompensa (o una penalización) en cada paso temporal.    \n",
    "El agente utiliza la RL para averiguar la política óptima que maximiza las recompensas totales que gana el agente. \n",
    "\n",
    "### Componentes de un entorno de RL   \n",
    "\n",
    "Los siguientes elementos son los componentes clave de un entorno de RL: \n",
    "\n",
    "- **Entorno**: El sistema, mundo o contexto externo. El agente interactúa con el entorno en una serie de pasos temporales. En cada paso temporal, en función de la acción del agente, el entorno:\n",
    "    - Da una recompensa (o penalización) \n",
    "    - Decide el siguiente estado \n",
    "- **Estado**: Una representación matemática de la configuración actual del entorno. \n",
    "    - Por ejemplo, el estado de un entorno pendular puede incluir la posición y la velocidad angular del péndulo en cada paso de tiempo. \n",
    "    - Estado terminal: Un estado que no conduce a nuevos/otros estados. \n",
    "- **Agente**: El algoritmo que observa el entorno y realiza diversas acciones basándose en esta observación. El objetivo del agente es maximizar sus recompensas. \n",
    "    - Por ejemplo, el agente decide con qué fuerza y en qué dirección empujar el péndulo.  \n",
    "- **Observación**: Una representación matemática de la visión que tiene el agente del entorno, adquirida, por ejemplo, mediante sensores. \n",
    "- **Acción**: La decisión que toma el agente antes de pasar al siguiente paso. La acción afecta al siguiente estado del entorno y hace que el agente obtenga una recompensa. \n",
    "- **Recompensa**: La retroalimentación del entorno al agente. Puede ser positivo o negativo, según la acción y el estado del entorno. \n",
    "- **Retorno**: La rentabilidad acumulada esperada en futuros periodos de tiempo. Las recompensas de pasos temporales futuros pueden descontarse utilizando un factor de descuento. \n",
    "- **Política**: La estrategia del agente sobre qué acción realizar en distintos estados. Se suele representar como una matriz de probabilidad,  P, que asigna estados a acciones.\n",
    "    - Dado un conjunto finito de m estados posibles y n acciones posibles, elemento Pmn de la matriz denota la probabilidad de tomar la acción an en el estado sm.  \n",
    "- **Episodio**: La serie de pasos temporales desde el estado inicial (aleatorio) hasta que el agente alcanza un estado terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Espacio de observación y espacio de acción   \n",
    "\n",
    "La observación es la información que el agente recoge sobre el entorno. Un agente, por ejemplo, un robot, podría recoger información del entorno mediante sensores. Lo ideal sería que el agente pudiera observar el estado completo, que describe todos los aspectos del entorno. En la práctica, el agente utiliza sus observaciones como sustituto del estado. Así, las observaciones deciden las acciones del agente. \n",
    "\n",
    "Un espacio es análogo a un conjunto matemático. El espacio de elementos  X incluye todas las instancias posibles de X. El espacio de  X también define la estructura (sintaxis y formato) de todos los elementos de tipo X. Cada entorno del Gimnasio tiene dos espacios, el espacio de acción, action_space, y el espacio de observación, observation_space. Tanto el espacio de acción como el de observación derivan de la superclase  padre gymnasium.spaces.Space. \n",
    "\n",
    "### Espacio de observación     \n",
    "\n",
    "El espacio de observación es el espacio que incluye todas las observaciones posibles. También define el formato en el que se almacenan las observaciones.    \n",
    "El espacio de observación suele representarse como un objeto de datos **Box**. Es un **ndarray** que describe los parámetros de las observaciones. La casilla especifica los límites de cada dimensión. Se puede ver el espacio de observación de un entorno utilizando el método `observation_space`, de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space:  Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"observation space: \", env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso del entorno **CartPole-v1**, la salida tiene el aspecto del ejemplo anterior.     \n",
    "\n",
    "En este ejemplo, el espacio de observación CartPole-v1 tiene 4 dimensiones. Los 4 elementos de la matriz de observación son\n",
    "\n",
    "- Posición del carro - varía entre -4,8 y +4,8\n",
    "- Velocidad del carro - oscila entre - a +\n",
    "- El ángulo del poste - varía entre -0,4189 y +0,4189\n",
    "- La velocidad angular del poste - oscila entre  - a +   \n",
    "\n",
    "Para ver un ejemplo de una matriz de observación individual, se utiliza el comando  `.reset()` . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation:  [ 0.03337675 -0.00124457 -0.01128544  0.02103772]\n"
     ]
    }
   ],
   "source": [
    "observation, info = env.reset()\n",
    "print(\"observation: \", observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los cuatro elementos de esta matriz se corresponden a las cuatro magnitudes observadas (posición del carro, velocidad del carro, ángulo del poste, velocidad angular del poste ), tal y como se ha visto previamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Espacio de acción   \n",
    "\n",
    "El **espacio de acción** incluye todas las acciones posibles que puede realizar el agente. El espacio de acción también define el formato en el que se representan las acciones.    \n",
    "Se puede ver el espacio de acción de un entorno utilizando el método `action_space`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space:  Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(\"action space: \", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso del entorno *\"CartPole-v1\"*, el espacio de acción es discreto. El agente puede realizar un total de dos acciones:\n",
    "\n",
    "- 0: Empuja el carro hacia la izquierda\n",
    "- 1: Empuja el carro hacia la derecha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construyendo nuestro primer agente para RL con Gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta este punto, se ha explorado los conceptos básicos de RL y Gymnasium. Ahora se mostrará cómo utilizar Gymnasium para construir un agente RL. \n",
    "\n",
    "### Crear y restablecer el entorno   \n",
    "\n",
    "El primer paso es crear una **instancia del entorno**. Para crear nuevos entornos, se utiliza el método `.make()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las interacciones del agente modifican el estado del entorno. El método `.reset()` restablece el entorno a un estado inicial.    \n",
    "Por defecto, el entorno se inicializa con un estado aleatorio. Se puede utilizar un parámetro `SEED` con el método `.reset()` para inicializar el entorno al mismo estado cada vez que se ejecute el programa.    \n",
    "El código siguiente muestra cómo hacerlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.0273956 , -0.00611216,  0.03585979,  0.0197368 ], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 42\n",
    "env.reset(seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El muestreo de acciones también implica aleatoriedad. Para controlar esta aleatoriedad y obtener una trayectoria de entrenamiento totalmente reproducible, es posible sembrar los generadores aleatorios de NumPy y/o PyTorch, tal y como se muesta a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f349f517ef0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acciones aleatorias frente a acciones inteligentes.   \n",
    "\n",
    "En cada paso de un proceso de Markov, el agente puede elegir aleatoriamente una acción y explorar el entorno hasta llegar a un estado terminal. Eligiendo acciones al azar: \n",
    "\n",
    "- Puede llevar mucho tiempo alcanzar el estado terminal.\n",
    "- Las recompensas acumuladas son mucho menores de lo que podrían haber sido.   \n",
    "\n",
    "Entrenar al agente para que optimice la selección de acciones basándose en experiencias anteriores (de interacción con el entorno) es más eficaz para maximizar las recompensas a largo plazo. \n",
    "\n",
    "El agente no entrenado comienza con acciones aleatorias basadas en una política inicializada aleatoriamente. Esta política suele representarse como una red neuronal. Durante el entrenamiento, el agente aprende la política óptima que maximiza las recompensas. En el RL, el proceso de entrenamiento también se denomina **optimización de la política**. \n",
    "\n",
    "Existen varios métodos de optimización de la política. Las **ecuaciones de Bellman** describen cómo calcular el valor de las políticas de RL y determinar la política óptima. En notebook, se utilizará una técnica llamada **gradientes de política (policy gradients)**. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación de un agente de gradiente de política simple   \n",
    "\n",
    "Para construir un agente de RL que utilice gradientes de política, se crea una red neuronal para aplicar la política, se escriben las funciones para calcular los rendimientos y las pérdidas a partir de las recompensas escalonadas y las probabilidades de acción, y se actualiza iterativamente la política mediante técnicas estándar de retropropagación. \n",
    "\n",
    "#### Configurar la red política\n",
    "Se usa una red neuronal para aplicar la política. Como \"CartPole-v1\" es un entorno sencillo, utilizamos una red neuronal con las siguientes características:\n",
    "\n",
    "- Dimensiones de entrada iguales a la dimensionalidad del espacio de observación del entorno. \n",
    "- Una sola capa oculta con 64 neuronas. \n",
    "- Dimensiones de salida iguales a la dimensionalidad del espacio de acción del entorno.   \n",
    "\n",
    "Por tanto, la función de la 'red de políticas' es asignar los estados observados a las acciones. Dada una observación de entrada, predice la acción correcta.    \n",
    "\n",
    "El código siguiente implementa la red de políticas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  La recogida de recompensas y el pase hacia delante\n",
    "\n",
    "En cada paso del proceso de Markov, el entorno da una recompensa basada en la acción y el estado del agente. El objetivo en RL es maximizar el rendimiento total. \n",
    "\n",
    "- El rendimiento en cada paso temporal es la suma acumulada de las recompensas obtenidas desde el principio hasta ese paso. \n",
    "- El rendimiento total en cada episodio se obtiene acumulando todas las recompensas escalonadas de ese episodio. Así, el rendimiento total es el rendimiento en el último paso temporal (cuando el agente alcanza un estado terminal).    \n",
    "\n",
    "En la práctica, al acumular recompensas, es habitual: \n",
    "\n",
    "- Ajustar las recompensas futuras utilizando un factor de descuento. \n",
    "- Normalizar el conjunto de retornos escalonados para garantizar un entrenamiento suave y estable.    \n",
    "\n",
    "\n",
    "El código siguiente muestra cómo hacerlo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stepwise_returns(rewards, discount_factor):\n",
    "    returns = []\n",
    "    R = 0\n",
    "    for r in reversed(rewards):\n",
    "        R = r + R * discount_factor\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns)\n",
    "    normalized_returns = (returns - returns.mean()) / returns.std()\n",
    "    return normalized_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El paso hacia delante consiste en hacer funcionar al agente basándose en la política actual hasta que alcance un estado terminal y recoger las recompensas y probabilidades de acción escalonadas. Los pasos siguientes explican cómo aplicar el pase hacia delante: \n",
    "\n",
    "- Restablece el entorno a un estado inicial. \n",
    "- Inicializa los búferes para almacenar las probabilidades de acción, las recompensas y el rendimiento acumulado\n",
    "- Utiliza la función `.step()` para ejecutar iterativamente el agente en el entorno hasta que termine:\n",
    "    - Obtén la observación del estado del entorno.\n",
    "    - Obtén la acción prevista por la política en función de la observación.\n",
    "    - Utiliza la función `Softmax` para estimar la probabilidad de realizar la acción prevista.\n",
    "    - Simula una distribución de probabilidad categórica basada en estas probabilidades estimadas.\n",
    "    - Muestrea esta distribución para obtener la acción del agente.\n",
    "    - Estima la probabilidad logarítmica de la acción muestreada a partir de la distribución simulada. \n",
    "- Añade la probabilidad logarítmica de las acciones y las recompensas de cada paso a sus respectivos búferes. \n",
    "- Estima los valores normalizados y descontados de los rendimientos en cada paso en función de las recompensas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import distributions\n",
    "def forward_pass(env, policy, discount_factor):\n",
    "    log_prob_actions = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "    episode_return = 0\n",
    "    policy.train()\n",
    "    observation, info = env.reset()\n",
    "    while not done:\n",
    "        observation = torch.FloatTensor(observation).unsqueeze(0)\n",
    "        action_pred = policy(observation)\n",
    "        action_prob = F.softmax(action_pred, dim = -1)\n",
    "        dist = distributions.Categorical(action_prob)\n",
    "        action = dist.sample()\n",
    "        log_prob_action = dist.log_prob(action)\n",
    "        observation, reward, terminated, truncated, info = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "        log_prob_actions.append(log_prob_action)\n",
    "        rewards.append(reward)\n",
    "        episode_return += reward\n",
    "    log_prob_actions = torch.cat(log_prob_actions)\n",
    "    stepwise_returns = calculate_stepwise_returns(rewards, discount_factor)\n",
    "    return episode_return, stepwise_returns, log_prob_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actualizar la política en función de las recompensas\n",
    "La pérdida representa la cantidad sobre la que aplicamos el descenso gradiente. El objetivo en RL es maximizar los beneficios. Por lo tanto, utilizamos el valor de rendimiento esperado como sustituto de la pérdida.    \n",
    "El valor de rendimiento esperado se calcula como el producto de los rendimientos esperados escalonados y la probabilidad logarítmica de las acciones escalonadas.    \n",
    "\n",
    "El código siguiente calcula la pérdida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(stepwise_returns, log_prob_actions):\n",
    "    loss = -(stepwise_returns * log_prob_actions).sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para actualizar la política, ejecuta la retropropagación con respecto a la función de pérdida. El siguiente método `update_policy()` invoca al método `calculate_loss()`. A continuación, ejecuta la retropropagación sobre esta pérdida para actualizar los parámetros de la política, es decir, los pesos del modelo de la red de políticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(stepwise_returns, log_prob_actions, optimizer):\n",
    "    stepwise_returns = stepwise_returns.detach()\n",
    "    loss = calculate_loss(stepwise_returns, log_prob_actions)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La actualización de la política basada en el gradiente de los rendimientos se denomina método del gradiente de la política. \n",
    "\n",
    "### Formación de la política   \n",
    "\n",
    "Ahora tenemos todos los componentes necesarios para formar y evaluar la política. Ejecutamos el bucle de entrenamiento como se explica en los pasos siguientes:  \n",
    "\n",
    "Antes de empezar, declaramos los hiperparámetros, instanciamos una política y creamos un optimizador:\n",
    "\n",
    "- Declara los hiperparámetros como constantes de Python:\n",
    "    - MAX_EPOCHS es el número máximo de iteraciones que estamos dispuestos a realizar para entrenar la política. \n",
    "    - DISCOUNT_FACTOR decide la importancia relativa de las recompensas de los pasos temporales futuros. Un factor de descuento de 1 significa que todas las recompensas tienen la misma importancia, mientras que un valor de 0 significa que sólo es importante la recompensa del paso temporal actual. \n",
    "    - N_TRIALS es el número de episodios sobre los que promediamos los rendimientos para evaluar el rendimiento del agente. Decidimos que el entrenamiento ha tenido éxito si el rendimiento medio de N_TRIALS episodios está por encima del umbral . \n",
    "    - REWARD_THRESHOLD: Si la política consigue un rendimiento superior al umbral, se considera que ha tenido éxito. \n",
    "    - DROPOUT decide la fracción de los pesos que deben ponerse a cero aleatoriamente. La función de abandono pone a cero aleatoriamente una fracción de las ponderaciones del modelo. Esto reduce la dependencia de neuronas específicas y evita el sobreajuste, haciendo que la red sea más robusta.\n",
    "    - LEARNING_RATE decide cuánto se pueden modificar los parámetros de la política en cada paso. La actualización de los parámetros en cada iteración es el producto del gradiente y la tasa de aprendizaje. \n",
    "- Definir la política como una instancia de la clase PolicyNetwork (implementada anteriormente). \n",
    "- Crear un optimizador utilizando el algoritmo Adam y la tasa de aprendizaje.    \n",
    "\n",
    "Para entrenar la política, ejecutamos iterativamente los pasos de entrenamiento hasta que el rendimiento medio (sobre N_TRIALS ) sea mayor que el umbral de recompensa:\n",
    "\n",
    "- Para cada episodio, ejecuta el paso adelante una vez. Recoge la probabilidad logarítmica de las acciones, los rendimientos escalonados y el rendimiento total de ese episodio. Acumula los rendimientos episódicos en una matriz. \n",
    "- Calcular la pérdida utilizando las probabilidades logarítmicas y los retornos escalonados. Ejecuta la retropropagación sobre la pérdida. Utiliza el optimizador para actualizar los parámetros de la política. \n",
    "- Comprobar si la rentabilidad media en N_TRIALS supera el umbral de recompensa.   \n",
    " \n",
    "El código siguiente implementa estos pasos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "def main(): \n",
    "    MAX_EPOCHS = 500\n",
    "    DISCOUNT_FACTOR = 0.99\n",
    "    N_TRIALS = 25\n",
    "    REWARD_THRESHOLD = 475\n",
    "    PRINT_INTERVAL = 10\n",
    "    INPUT_DIM = env.observation_space.shape[0]\n",
    "    HIDDEN_DIM = 128\n",
    "    OUTPUT_DIM = env.action_space.n\n",
    "    DROPOUT = 0.5\n",
    "    episode_returns = []\n",
    "    policy = PolicyNetwork(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM, DROPOUT)\n",
    "    LEARNING_RATE = 0.01\n",
    "    optimizer = optim.Adam(policy.parameters(), lr = LEARNING_RATE)\n",
    "    for episode in range(1, MAX_EPOCHS+1):\n",
    "        episode_return, stepwise_returns, log_prob_actions = forward_pass(env, policy, DISCOUNT_FACTOR)\n",
    "        _ = update_policy(stepwise_returns, log_prob_actions, optimizer)\n",
    "        episode_returns.append(episode_return)\n",
    "        mean_episode_return = np.mean(episode_returns[-N_TRIALS:])\n",
    "        if episode % PRINT_INTERVAL == 0:\n",
    "            print(f'| Episode: {episode:3} | Mean Rewards: {mean_episode_return:5.1f} |')\n",
    "        if mean_episode_return >= REWARD_THRESHOLD:\n",
    "            print(f'Reached reward threshold in {episode} episodes')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se invoca la función `main()`para ejecutar el entrenamiento de la red de políticas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Episode:  10 | Mean Rewards:  27.8 |\n",
      "| Episode:  20 | Mean Rewards:  26.9 |\n",
      "| Episode:  30 | Mean Rewards:  31.9 |\n",
      "| Episode:  40 | Mean Rewards:  35.5 |\n",
      "| Episode:  50 | Mean Rewards:  53.8 |\n",
      "| Episode:  60 | Mean Rewards:  67.6 |\n",
      "| Episode:  70 | Mean Rewards:  57.4 |\n",
      "| Episode:  80 | Mean Rewards:  33.3 |\n",
      "| Episode:  90 | Mean Rewards:  31.3 |\n",
      "| Episode: 100 | Mean Rewards:  41.0 |\n",
      "| Episode: 110 | Mean Rewards:  63.8 |\n",
      "| Episode: 120 | Mean Rewards:  65.9 |\n",
      "| Episode: 130 | Mean Rewards:  75.7 |\n",
      "| Episode: 140 | Mean Rewards: 211.8 |\n",
      "| Episode: 150 | Mean Rewards: 286.2 |\n",
      "| Episode: 160 | Mean Rewards: 233.9 |\n",
      "| Episode: 170 | Mean Rewards: 209.5 |\n",
      "| Episode: 180 | Mean Rewards: 163.0 |\n",
      "| Episode: 190 | Mean Rewards:  66.9 |\n",
      "| Episode: 200 | Mean Rewards:  10.5 |\n",
      "| Episode: 210 | Mean Rewards:  10.6 |\n",
      "| Episode: 220 | Mean Rewards:  11.6 |\n",
      "| Episode: 230 | Mean Rewards:  12.6 |\n",
      "| Episode: 240 | Mean Rewards:  13.8 |\n",
      "| Episode: 250 | Mean Rewards:  13.5 |\n",
      "| Episode: 260 | Mean Rewards:  17.0 |\n",
      "| Episode: 270 | Mean Rewards:  22.4 |\n",
      "| Episode: 280 | Mean Rewards:  60.1 |\n",
      "| Episode: 290 | Mean Rewards: 198.7 |\n",
      "| Episode: 300 | Mean Rewards: 370.4 |\n",
      "| Episode: 310 | Mean Rewards: 448.7 |\n",
      "| Episode: 320 | Mean Rewards: 448.7 |\n",
      "| Episode: 330 | Mean Rewards: 471.0 |\n",
      "Reached reward threshold in 334 episodes\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Técnicas Avanzadas con **Gymnasium**   \n",
    "\n",
    "Una vez se ha puesto en práctica un algoritmo de RL, ahora se verán de algunas técnicas avanzadas que se utilizan habitualmente en la práctica. \n",
    "\n",
    "### Utilizar arquitecturas preconstruidas.   \n",
    "\n",
    "Implementar algoritmos de RL desde cero es un proceso largo y difícil, sobre todo para entornos complejos y políticas de última generación. \n",
    "\n",
    "Una alternativa más práctica es utilizar programas como [Stable-BaseLines3](https://stable-baselines3.readthedocs.io/en/master/). Viene con implementaciones probadas de algoritmos de RL. Incluye agentes preentrenados, guiones de entrenamiento, herramientas de evaluación y módulos para trazar gráficos y grabar vídeos. \n",
    "\n",
    "[Ray RLib](https://docs.ray.io/en/latest/rllib/index.html) es otra herramienta popular para RL. RLib está diseñado como una solución escalable, que facilita la implementación de algoritmos RL en sistemas multi-GPU. También admite RL multiagente, lo que abre nuevas posibilidades como:\n",
    "\n",
    "- Aprendizaje multiagente independiente: Cada agente trata a los demás agentes como parte del entorno.\n",
    "- Entrenamiento multiagente colaborativo: Un grupo de agentes comparten la misma política y funciones de valor y aprenden paralelamente de las experiencias de los demás. \n",
    "- Formación adversaria: Los agentes (o grupos de agentes) compiten entre sí en entornos competitivos similares a un juego. \n",
    "\n",
    "Tanto con **RLib** como con **Stable-Baselines3**, se puede importar y utilizar entornos de Gymnasium. \n",
    "\n",
    "### Entornos personalizados   \n",
    "\n",
    "Los entornos empaquetados con Gymnasium son la elección adecuada para probar nuevas estrategias de RL y políticas de formación. Sin embargo, para la mayoría de las aplicaciones prácticas, es necesario crear y utilizar un entorno que refleje fielmente el problema que se desea resolver. Es posible utilizar Gymnasium para crear un entorno personalizado.    \n",
    "La ventaja de utilizar los entornos personalizados de Gymnasium es que muchas herramientas externas como RLib y Stable-Baselines3 ya están configuradas para trabajar con la estructura de la API de Gymnasium. \n",
    "\n",
    "Para crear un entorno personalizado en Gimnasio, tienes que definir: \n",
    "\n",
    "- El espacio de observación.\n",
    "- Las condiciones terminales.\n",
    "- El conjunto de acciones que el agente puede elegir.\n",
    "- Cómo inicializar el entorno (cuando se llama a la función `reset()`) . \n",
    "- Cómo decide el entorno el siguiente estado dadas las acciones del agente (cuando se llama a la función `step()`).   \n",
    "\n",
    "Para saber más, es posible usar la [guía de Gymnasium sobre la creación de entornos personalizados](https://gymnasium.farama.org/introduction/create_custom_env/). \n",
    "\n",
    "### Buenas prácticas para utilizar Gymnasium   \n",
    "\n",
    "#### Experimentar con distintos entornos. \n",
    "\n",
    "El código de este notebook muestra cómo aplicar el algoritmo **'policy gradient'** en el entorno *\"CartPole\"*. Se trata de un entorno sencillo con un espacio de acción discreto. Para comprender mejor la RL, es aconsejable aplicar este mismo algoritmo **'policy gradient'** en otros entornos .\n",
    "\n",
    "Por ejemplo, el entorno **\"Péndulo\"(\"Pendulumn-V1\")** tiene un espacio de acción continuo. Consiste en una única entrada representada como variable continua: el par magnitud y dirección aplicado al péndulo en cualquier estado dado. Este par puede tomar cualquier valor entre -2 y +2. \n",
    "\n",
    "Experimentar con distintos algoritmos en diversos entornos te ayuda a comprender mejor los distintos tipos de soluciones de RL y sus retos. \n",
    "\n",
    "#### Supervisar el progreso de la formación/entrenamiento del agente     \n",
    "\n",
    "Los entornos de RL suelen consistir en robots, péndulos, coches de montaña, videojuegos, etc. Visualizar las acciones del agente en el entorno permite comprender mejor e intuitivamente el rendimiento de la política. \n",
    "\n",
    "En Gymnasium, el método `env.render()` visualiza las interacciones del agente con el entorno.    \n",
    "Muestra gráficamente el estado actual del entorno: las pantallas del juego, la posición del péndulo o del poste del carro, etc. La información visual de las acciones del agente y de las respuestas del entorno ayudan a controlar el rendimiento del agente y su progreso en el proceso de entrenamiento. \n",
    "\n",
    "Hay cuatro modos de renderizado: \"humano\"(\"human\"), \"rgb_array\", \"ansi\" y \"rgb_array_list\". Para visualizar el rendimiento del agente, utiliza el modo de renderizado \"humano\"(\"human\"). El modo de renderizado se especifica cuando se inicializa el entorno. Por ejemplo:   \n",
    "\n",
    "`env = gym.make(‘CartPole-v1’, render_mode=’human’)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar la renderización, se invoca al método `.render()` después de cada acción realizada por el agente (mediante la llamada al método `.step()`).    \n",
    "\n",
    "El pseudocódigo siguiente ilustra cómo hacerlo:   \n",
    "````\n",
    "while not done:\n",
    "    …\n",
    "   step, reward, terminated, truncated, info = env.step(action.item())\n",
    "   env.render()\n",
    "    …\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solución de errores comunes.   \n",
    "\n",
    "Gymnasium facilita la interfaz con entornos RL complejos. Sin embargo, es un software continuamente actualizado con muchas dependencias. Por eso, es esencial estar atento a algunos tipos comunes de errores.\n",
    "\n",
    "### Desajustes de versión\n",
    "- **No coincide la versión de Gymnasium**: El paquete de software Gymnasium de Farama fue bifurcado de Gymnasium de OpenAI a partir de la versión 0.26.2. Se han producido algunos cambios de última hora entre las versiones anteriores de Gymnasium y las nuevas versiones de Gymnasium. Muchas implementaciones disponibles públicamente se basan en las versiones anteriores de Gym y puede que no funcionen directamente con la última versión. En estos casos, es necesario revertir la instalación a una versión anterior o adaptar el código para que funcione con la nueva versión. \n",
    "- **No coincide la versión del entorno**: Muchos entornos de Gymnasium tienen versiones diferentes. Por ejemplo, hay dos entornos CartPole: CartPole-v1 y CartPole-v0. Aunque el comportamiento del entorno es el mismo en ambas versiones, algunos de los parámetros, como la duración del episodio, el umbral de recompensa, etc., pueden ser diferentes. Una política entrenada en una versión puede no funcionar tan bien en otra versión del mismo entorno. Tienes que actualizar los parámetros de entrenamiento y volver a entrenar la política para cada versión del entorno. \n",
    "- **No coincide la versión de las dependencias**: Gymnasium depende de dependencias como NumPy y PyTorch. En diciembre de 2024, las últimas versiones de estas dependencias son numpy 2.1.3 y torch 2.5.1. Sin embargo, Gymnasium funciona mejor con torch 1.13.0 y numpy 1.23.3. Puedes encontrar problemas si instalas Gymnasium en un entorno con estas dependencias preinstaladas. Recomendamos instalar y trabajar con Gymnasium en un entorno Conda nuevo.    \n",
    "\n",
    "### Problemas de convergencia\n",
    "- **Hiperparámetros**: Al igual que otros algoritmos de aprendizaje automático, las políticas de RL son sensibles a hiperparámetros como la tasa de aprendizaje, el factor de descuento, etc. Es recomendamble experimentar y ajustar los hiperparámetros manualmente o utilizando técnicas automatizadas como la búsqueda en cuadrícula y la búsqueda aleatoria. \n",
    "- **Exploración frente a explotación**: Para algunas clases de políticas (como la PPO), el agente adopta una estrategia doble: explorar el entorno para descubrir nuevos caminos y adoptar un enfoque codicioso para maximizar las recompensas basándose en los caminos conocidos hasta el momento. Si explora demasiado, la política no converge. A la inversa, nunca intenta el camino óptimo si no explora lo suficiente. Por tanto, encontrar el equilibrio adecuado entre exploración y explotación es esencial. También es habitual dar prioridad a la exploración en los primeros episodios y a la explotación en los últimos durante el entrenamiento.    \n",
    "\n",
    "### Inestabilidad del entrenamiento\n",
    "- **Grandes ritmos de aprendizaje**: Si la tasa de aprendizaje es demasiado alta, los parámetros de la política sufren grandes actualizaciones en cada paso. Esto podría hacer que no se alcanzara el conjunto óptimo de valores. Una solución habitual es disminuir gradualmente la tasa de aprendizaje, garantizando actualizaciones más pequeñas y estables a medida que converge el entrenamiento. \n",
    "- **Exploración excesiva**: Demasiada aleatoriedad (entropía) en la selección de acciones impide la convergencia y provoca grandes variaciones en la función de pérdida entre los pasos siguientes. Para tener un proceso de entrenamiento estable y convergente, equilibra la exploración con la explotación. \n",
    "- **Elección incorrecta del algoritmo**: Los algoritmos simples, como el policy gradient, pueden conducir a un entrenamiento inestable en entornos complejos con grandes espacios de acción y de estado. En estos casos, recomendamos utilizar algoritmos más robustos, como PPO y Optimización de la Política de la Región de Confianza (TRPO). Estos algoritmos evitan grandes actualizaciones de la política en cada paso y pueden ser más estables.\n",
    "- **Aleatoriedad**: Los algoritmos de RL son notoriamente sensibles a los estados iniciales y a la aleatoriedad inherente a la selección de acciones. Cuando una ejecución de entrenamiento es inestable, a veces puede estabilizarse utilizando una semilla aleatoria diferente o reinicializando la política."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7425342",
   "metadata": {},
   "source": [
    "# 1. Introducción al Aprendizaje por Refuerzo (Reinforcement Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99f640b",
   "metadata": {},
   "source": [
    "### 1.1 Los campos del Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24522b87",
   "metadata": {},
   "source": [
    "Existen 3 grandes campos:\n",
    "\n",
    "- **Aprendizaje no Supervisado**: tenemos datos pero **NO** sabemos a qué grupo pertenece cada dato.\n",
    "- **Aprendizaje Supervisado**: tenemos datos y **SÍ** sabemos a qué grupo pertenece cada dato (etiquetas).\n",
    "- **Aprendizaje por Refuerzo**: no tenemos datos, los datos se obtienen explorando un entorno.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c3048d",
   "metadata": {},
   "source": [
    "![esquema_ML](https://la.mathworks.com/discovery/reinforcement-learning/_jcr_content/mainParsys3/discoverysubsection/mainParsys/image.adapt.full.medium.png/1630398182247.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b822ee",
   "metadata": {},
   "source": [
    "### 1.2 Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e46eff1",
   "metadata": {},
   "source": [
    "![Reinforcement Learning simple schema](https://la.mathworks.com/discovery/reinforcement-learning/_jcr_content/mainParsys3/discoverysubsection_603098216/mainParsys3/image.adapt.full.medium.png/1630398182451.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f3b0fb",
   "metadata": {},
   "source": [
    "#### CICLO DE APRENDIZAJE   \n",
    "\n",
    "1. El **agente** obtiene unas **observaciones** del entorno\n",
    "2. En función a esas **observaciones** decide realizar una **acción**\n",
    "3. Esa **acción** le lleva a obtener una **recompensa** y nuevas **observaciones**\n",
    "4. Vuelve al paso 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8cd723",
   "metadata": {},
   "source": [
    "#### ¿Cual sería el Agente, el entorno, las acciones, la recompensa y las observaciones en este clásico ejemplo?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ba0272",
   "metadata": {},
   "source": [
    "![mice in a maze](https://user-images.githubusercontent.com/44867923/139915800-8224bede-c52b-47d1-bb22-2e9624687831.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a802dc34",
   "metadata": {},
   "source": [
    "### 1.3 Obtener datos: Equilibrio entre exploración y explotación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f74ecea",
   "metadata": {},
   "source": [
    "1. Te acabas de mudar. No conoces nada de tu barrio.\n",
    "2. Tienes hambre (**recompensa** negativa) y quieres comer.\n",
    "3. Observas que existen varios restaurantes en tu calle (**observaciones**)\n",
    "4. Eliges un restaurante (tomas una **acción**), no conoces ninguno asi que cómo lo haces? **De forma aleatoria**\n",
    "5. Te gusta (**recompensa positiva**) y repites porque vas a lo seguro (**Explotación**).\n",
    "6. Cierto día decides arriesgar con un nuevo sitio (**Exploración**).\n",
    "7. Puede ser que te guste más y vuelvas (**Explotación**) o puede ser que no te guste y pruebes nuevos sitios (**Exploración**) o vuelvas al anterior (**Explotación**)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a025ca0",
   "metadata": {},
   "source": [
    "![explotationvsexploration](https://steemitimages.com/640x0/https://steemitimages.com/DQmXH5tjBiS41iNtcyvh7s7Rj5z3SqGkcwoaV2otRJNx3FT/Exploration_vs._Exploitation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbb7185",
   "metadata": {},
   "source": [
    "### 1.4 Objetivo del RL: Maximizar la recompensa a largo plazo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae07579",
   "metadata": {},
   "source": [
    "1. Madrugas (recompensa negativa)\n",
    "2. Vas a clase, *con una asignatura aburrida* (recompensa negativa)\n",
    "3. Te fuerzas a estudiar cientos de diapositivas con mucho texto (recompensa negativa).\n",
    "4. ¿Por qué? -> **Para maximizar la recompensa a largo plazo**.\n",
    "4. Si estudio, trabajaré en lo que me gusta. (recompensa positiva).\n",
    "6. Puedo aspirar a trabajos mejor remunerados (recompensa positiva).\n",
    "7. Con ese dinero puedo vivir en mejores condiciones(recompensa positiva)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfdd42a",
   "metadata": {},
   "source": [
    "#### Exploración vs. Explotación\n",
    "\n",
    "Supongamos que se desea enseñar al gato Bob a utilizar varios rascadores en una habitación en lugar de usar los muebles. Por tanto, Bob es el agente, el que aprende y el que toma las decisiones. Tiene que aprender qué cosas puede arañar (alfombras y postes) y cuáles no (sofás y cortinas).   \n",
    "\n",
    "Como un hábil jugador de ajedrez, nuestro agente buscará las recompensas más gratificantes. Esto pone de manifiesto un dilema clásico en el aprendizaje por refuerzo: **exploración** frente a **explotación**.\n",
    "\n",
    "Mientras que un puesto tentador puede ofrecer una gratificación inmediata, una exploración más estratégica podría conducir a un premio gordo más adelante. Al igual que un jugador de ajedrez puede renunciar a una captura para obtener una posición superior, nuestro agente puede optar inicialmente por un puesto subóptimo (exploración) para descubrir el refugio definitivo para rascar (explotación). Esta estrategia a largo plazo es crucial para que los agentes maximicen las recompensas en entornos complejos.\n",
    "\n",
    "En otras palabras, Bob debe equilibrar la **explotación** (ceñirse a lo que funciona mejor) con la **exploración** (aventurarse de vez en cuando a buscar nuevos rascadores). Explorar demasiado puede hacerle perder tiempo, sobre todo en entornos continuos, mientras que explotar demasiado puede hacer que Bob se pierda algo aún mejor.\n",
    "\n",
    "Por suerte, Bob puede adoptar algunas estrategias inteligentes:\n",
    "\n",
    "- *Aprendizaje Epsilon-greedy (voraz)*: Imaginemos por un momento que Bob tiene un \"rascador-metro\" especial que genera números aleatorios. Si el número generado es menor que algún umbral predefinido llamado épsilon, Bob prueba una superficie de rascado aleatoria (exploración). Pero si el número es mayor que épsilon, Bob va a por el puesto que mejor le parecía antes (explotación).\n",
    "- *Exploración de Boltzmann*: Si Bob sigue rascando cosas que no le parecen bien (obteniendo recompensas negativas), es más probable que explore nuevas opciones (aumento de la exploración). Pero cuando encuentre el rascador perfecto (recompensas positivas), se quedará en ese lugar feliz (explotación).   \n",
    "\n",
    "Utilizando estas estrategias (u otras), Bob puede encontrar un equilibrio entre explorar lo desconocido y ceñirse a lo bueno (explotar)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6527ebfa",
   "metadata": {},
   "source": [
    "### 1.5 Reinforcement Learning en la pŕactica\n",
    "\n",
    "[Interesante documental sobre AlphaGo](https://www.youtube.com/watch?v=GIJ7zr4sYx4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baf2144",
   "metadata": {},
   "source": [
    "#### Importar librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a03b87b",
   "metadata": {},
   "source": [
    "Instalación de las siguientes librerías:    \n",
    "\n",
    "- ```pip install swig```     \n",
    "- ```pip install gymnasium[box2d]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eef65dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6913cd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9fe062",
   "metadata": {},
   "source": [
    "#### Funciones para mostrar correctamente el entorno "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea06e2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para obtener la imagen del entorno correctamente\n",
    "def get_frame(env):\n",
    "    frame = env.render()\n",
    "    if isinstance(frame, dict):  # Algunas versiones de gymnasium devuelven un dict\n",
    "        return frame[\"pixels\"]\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe58e4f",
   "metadata": {},
   "source": [
    "#### Creación del entorno y juego aleatorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0404e5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de entornos\n",
    "ENV_NAMES = [\"MountainCar-v0\", \"LunarLander-v3\", \"CarRacing-v3\", \"BipedalWalker-v3\", \"CartPole-v1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc5e5791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entorno seleccionado: CarRacing-v3\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Seleccionar un entorno aleatorio\n",
    "env_name = random.choice(ENV_NAMES)\n",
    "\n",
    "print(f\"Entorno seleccionado: {env_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf54363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if env_name == \"CarRacing-v3\":\n",
    "    env = gym.make(env_name, render_mode=\"rgb_array\", continuous=True)  # el parámetro continuous es necesario para CarRacing, para que acepte acciones continuas.\n",
    "    env = gym.make(env_name, render_mode=\"rgb_array\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905e85e5",
   "metadata": {},
   "source": [
    "### Renderizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7d3debc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar render\n",
    "obs, _ = env.reset()\n",
    "done = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae33ce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar la visualización inicial\n",
    "plt.figure(figsize=(6, 4))\n",
    "img = plt.imshow(get_frame(env))  # Renderizar primer frame\n",
    "\n",
    "\n",
    "# Jugar una partida con una política aleatoria\n",
    "while not done:\n",
    "    action = env.action_space.sample()  # Selecciona una acción aleatoria\n",
    "    obs, reward, done, _, _ = env.step(action)  # Ejecutar acción\n",
    "\n",
    "    # Actualizar la imagen en Jupyter Notebook\n",
    "    img.set_data(get_frame(env))\n",
    "    display(plt.gcf())\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    time.sleep(0.05)  # Pausa para mejor visualización\n",
    "\n",
    "env.close()\n",
    "print(f\"Juego finalizado en {env_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "186430ca1b2c066b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## RED LSTM (LONG SHORT-TERM MEMORY - Memoria Larga a Corto Plazo) \n",
    "\n",
    "Este código crea un modelo LSTM que aprende a predecir el siguiente carácter en un texto dado basado en los cinco caracteres anteriores. Después de entrenar el modelo, genera texto automáticamente comenzando desde una secuencia inicial aleatoria.\n",
    "\n",
    "Este ejercicio permite demostrar cómo un LSTM puede aprender dependencias en datos secuenciales como el texto. Se usa un texto corto como ejemplo y se prepara el código para predecir el siguiente carácter dado una secuencia de caracteres anteriores.\n",
    "\n",
    "Se sigue una pauta similar a la llevada a cabo en el notebook 07_RNA_RedRNN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T10:52:04.450537Z",
     "start_time": "2024-05-17T10:51:59.528938Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 22:52:05.914033: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-19 22:52:05.958052: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742424726.007017   62089 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742424726.020697   62089 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-19 22:52:06.073006: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f9c484d16fa98",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Desactivamos warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51691c3fad23a893",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T10:52:18.308689Z",
     "start_time": "2024-05-17T10:52:18.305215Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21008b44ba716947",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Definición del conjunto de datos\n",
    "\n",
    "Para que un modelo de aprendizaje automático pueda procesar texto, primero debemos convertir los caracteres en una forma que el modelo pueda entender, es decir, números.   \n",
    "En este caso, tomamos la frase y generamos directamente el conjunto de caracteres que la forman y la convertimos a un conjunto de números enteros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddc4222fbd981a18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T10:52:20.343599Z",
     "start_time": "2024-05-17T10:52:20.340122Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Texto de ejemplo\n",
    "# text = \"fractured world tested the hope of a young president\"\n",
    "text = \"en un lugar de la mancha de cuyo nombre no quiero acordarme\"\n",
    "# Convertir caracteres a enteros\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dea2c5d58b5777",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Preparación de datos para ser usados por la red LSTM\n",
    "\n",
    "Un modelo LSTM necesita conocer no solo el estado actual (por ejemplo, un carácter en el texto), sino también algunos estados anteriores para hacer una predicción efectiva. Esto es lo que le permite aprender y entender el contexto o la secuencia en los datos.\n",
    "\n",
    "En este caso, se define una longitud de secuencia de 5 (seq_length). De esta forma, se va a utilizar secuencias de 5 caracteres para predecir el siguiente carácter.   \n",
    "   \n",
    "Creamos todas las posibles subsecuencias de 5 caracteres del texto y el carácter siguiente a cada subsecuencia será el **carácter objetivo** que el modelo intentará predecir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccb894f256bd6ad7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T10:52:23.574700Z",
     "start_time": "2024-05-17T10:52:23.569956Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preparar los datos para el LSTM\n",
    "seq_length = 5\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(text) - seq_length, 1):\n",
    "    seq_in = text[i:i + seq_length]\n",
    "    seq_out = text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff88cce8ad853bd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Reformateo de los Datos de Entrada para el LSTM\n",
    "Los modelos LSTM en TensorFlow esperan una entrada con la forma [muestras, pasos de tiempo, características]:\n",
    "\n",
    "- **Muestras**: Número total de secuencias de ejemplo.\n",
    "- **Pasos de tiempo**: Número de unidades de tiempo en la secuencia; en este caso, es la longitud de la secuencia de caracteres (5).\n",
    "- **Características**: Número de características en cada paso de tiempo; en este ejemplo, es 1 porque tenemos un carácter por paso de tiempo.\n",
    "\n",
    "Además, se normalizan los valores de entrada dividiendo por el número total de caracteres únicos para ayudar al modelo a converger más rápido durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d601a15d098b68f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T10:52:26.015208Z",
     "start_time": "2024-05-17T10:52:26.010836Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_patterns = len(dataX)\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "X = X / float(len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb8d3e27cdfb7bf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Codificación One-Hot para la Salida\n",
    "\n",
    "La salida se codifica como un vector one-hot, lo que significa que cada carácter posible se representa como un vector binario con un solo bit activo (1) y todos los demás inactivos (0). Esto es necesario para la clasificación, donde cada entrada puede ser clasificada como uno de los caracteres posibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "266b2365bc3ee2e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T10:52:28.653453Z",
     "start_time": "2024-05-17T10:52:28.648800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = tf.keras.utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c60ad859c288c7c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Definición de la arquitectura del modelo de red LSTM\n",
    "\n",
    "Definición y compilación del modelo de red LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83405f80cacca107",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T10:52:31.692245Z",
     "start_time": "2024-05-17T10:52:31.604920Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 22:53:09.453250: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/usr/local/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Definir el modelo LSTM\n",
    "model = Sequential([\n",
    "    LSTM(128, input_shape=(X.shape[1], X.shape[2]), return_sequences=True),\n",
    "    LSTM(128),\n",
    "    Dense(y.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e17104bc2cf523",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45bff0dc7f75435",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T10:52:54.080916Z",
     "start_time": "2024-05-17T10:52:35.899724Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 - 5s - 95ms/step - loss: 2.8368\n",
      "Epoch 2/100\n",
      "54/54 - 1s - 14ms/step - loss: 2.7292\n",
      "Epoch 3/100\n",
      "54/54 - 1s - 13ms/step - loss: 2.6668\n",
      "Epoch 4/100\n",
      "54/54 - 1s - 14ms/step - loss: 2.6630\n",
      "Epoch 5/100\n",
      "54/54 - 1s - 14ms/step - loss: 2.6381\n",
      "Epoch 6/100\n",
      "54/54 - 1s - 14ms/step - loss: 2.6517\n",
      "Epoch 7/100\n",
      "54/54 - 1s - 14ms/step - loss: 2.6301\n",
      "Epoch 8/100\n",
      "54/54 - 1s - 14ms/step - loss: 2.6164\n",
      "Epoch 9/100\n",
      "54/54 - 1s - 14ms/step - loss: 2.6264\n",
      "Epoch 10/100\n",
      "54/54 - 1s - 14ms/step - loss: 2.6300\n",
      "Epoch 11/100\n",
      "54/54 - 1s - 14ms/step - loss: 2.6402\n",
      "Epoch 12/100\n",
      "54/54 - 1s - 14ms/step - loss: 2.5949\n",
      "Epoch 13/100\n",
      "54/54 - 1s - 15ms/step - loss: 2.5992\n",
      "Epoch 14/100\n",
      "54/54 - 1s - 15ms/step - loss: 2.5532\n",
      "Epoch 15/100\n",
      "54/54 - 1s - 14ms/step - loss: 2.5237\n",
      "Epoch 16/100\n",
      "54/54 - 1s - 14ms/step - loss: 2.4653\n",
      "Epoch 17/100\n",
      "54/54 - 1s - 14ms/step - loss: 2.4586\n",
      "Epoch 18/100\n",
      "54/54 - 1s - 13ms/step - loss: 2.4234\n",
      "Epoch 19/100\n",
      "54/54 - 1s - 14ms/step - loss: 2.3856\n",
      "Epoch 20/100\n",
      "54/54 - 1s - 14ms/step - loss: 2.3610\n",
      "Epoch 21/100\n",
      "54/54 - 1s - 13ms/step - loss: 2.2905\n",
      "Epoch 22/100\n",
      "54/54 - 1s - 14ms/step - loss: 2.2378\n",
      "Epoch 23/100\n",
      "54/54 - 1s - 13ms/step - loss: 2.2744\n",
      "Epoch 24/100\n",
      "54/54 - 1s - 13ms/step - loss: 2.2649\n",
      "Epoch 25/100\n",
      "54/54 - 1s - 14ms/step - loss: 2.1392\n",
      "Epoch 26/100\n",
      "54/54 - 1s - 13ms/step - loss: 2.1791\n",
      "Epoch 27/100\n",
      "54/54 - 1s - 14ms/step - loss: 2.0972\n",
      "Epoch 28/100\n",
      "54/54 - 1s - 14ms/step - loss: 2.0655\n",
      "Epoch 29/100\n",
      "54/54 - 1s - 14ms/step - loss: 2.0316\n",
      "Epoch 30/100\n",
      "54/54 - 1s - 13ms/step - loss: 2.0421\n",
      "Epoch 31/100\n",
      "54/54 - 1s - 13ms/step - loss: 1.9062\n",
      "Epoch 32/100\n",
      "54/54 - 1s - 14ms/step - loss: 2.0071\n",
      "Epoch 33/100\n",
      "54/54 - 1s - 14ms/step - loss: 1.9302\n",
      "Epoch 34/100\n",
      "54/54 - 1s - 14ms/step - loss: 1.9056\n",
      "Epoch 35/100\n",
      "54/54 - 1s - 14ms/step - loss: 1.7982\n",
      "Epoch 36/100\n",
      "54/54 - 1s - 14ms/step - loss: 1.7928\n",
      "Epoch 37/100\n",
      "54/54 - 1s - 13ms/step - loss: 1.8513\n",
      "Epoch 38/100\n",
      "54/54 - 1s - 15ms/step - loss: 1.6888\n",
      "Epoch 39/100\n",
      "54/54 - 1s - 15ms/step - loss: 1.6034\n",
      "Epoch 40/100\n",
      "54/54 - 1s - 14ms/step - loss: 1.5197\n",
      "Epoch 41/100\n",
      "54/54 - 1s - 13ms/step - loss: 1.5407\n",
      "Epoch 42/100\n",
      "54/54 - 1s - 14ms/step - loss: 1.4698\n",
      "Epoch 43/100\n",
      "54/54 - 1s - 15ms/step - loss: 1.4220\n",
      "Epoch 44/100\n",
      "54/54 - 1s - 14ms/step - loss: 1.3884\n",
      "Epoch 45/100\n",
      "54/54 - 1s - 13ms/step - loss: 1.3841\n",
      "Epoch 46/100\n",
      "54/54 - 1s - 13ms/step - loss: 1.3302\n",
      "Epoch 47/100\n",
      "54/54 - 1s - 14ms/step - loss: 1.2423\n",
      "Epoch 48/100\n",
      "54/54 - 1s - 14ms/step - loss: 1.1922\n",
      "Epoch 49/100\n",
      "54/54 - 1s - 13ms/step - loss: 1.2487\n",
      "Epoch 50/100\n",
      "54/54 - 1s - 14ms/step - loss: 1.1611\n",
      "Epoch 51/100\n",
      "54/54 - 1s - 14ms/step - loss: 1.0384\n",
      "Epoch 52/100\n",
      "54/54 - 1s - 14ms/step - loss: 1.0732\n",
      "Epoch 53/100\n",
      "54/54 - 1s - 14ms/step - loss: 1.1073\n",
      "Epoch 54/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.9690\n",
      "Epoch 55/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.9407\n",
      "Epoch 56/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.9490\n",
      "Epoch 57/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.8786\n",
      "Epoch 58/100\n",
      "54/54 - 1s - 13ms/step - loss: 0.8052\n",
      "Epoch 59/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.7401\n",
      "Epoch 60/100\n",
      "54/54 - 1s - 13ms/step - loss: 0.7288\n",
      "Epoch 61/100\n",
      "54/54 - 1s - 13ms/step - loss: 0.6595\n",
      "Epoch 62/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.6518\n",
      "Epoch 63/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.7628\n",
      "Epoch 64/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.8071\n",
      "Epoch 65/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.8184\n",
      "Epoch 66/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.9602\n",
      "Epoch 67/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.8665\n",
      "Epoch 68/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.6596\n",
      "Epoch 69/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.5090\n",
      "Epoch 70/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.5527\n",
      "Epoch 71/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.4832\n",
      "Epoch 72/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.4278\n",
      "Epoch 73/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.4344\n",
      "Epoch 74/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.4169\n",
      "Epoch 75/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.4148\n",
      "Epoch 76/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.4553\n",
      "Epoch 77/100\n",
      "54/54 - 1s - 13ms/step - loss: 0.4744\n",
      "Epoch 78/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.4968\n",
      "Epoch 79/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.3712\n",
      "Epoch 80/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.4625\n",
      "Epoch 81/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.5149\n",
      "Epoch 82/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.4007\n",
      "Epoch 83/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.3350\n",
      "Epoch 84/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.2555\n",
      "Epoch 85/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.3138\n",
      "Epoch 86/100\n",
      "54/54 - 1s - 13ms/step - loss: 0.2998\n",
      "Epoch 87/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.2272\n",
      "Epoch 88/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.2010\n",
      "Epoch 89/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.2430\n",
      "Epoch 90/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.2200\n",
      "Epoch 91/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.2164\n",
      "Epoch 92/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.2878\n",
      "Epoch 93/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.5244\n",
      "Epoch 94/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.6250\n",
      "Epoch 95/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.6951\n",
      "Epoch 96/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.3084\n",
      "Epoch 97/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.1828\n",
      "Epoch 98/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.1518\n",
      "Epoch 99/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.1454\n",
      "Epoch 100/100\n",
      "54/54 - 1s - 14ms/step - loss: 0.1459\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fbb855ab510>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "model.fit(X, y, epochs=100, batch_size=1, verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634a77049d4c5273",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Pruebas y predicción\n",
    "\n",
    "Primero, el código selecciona una secuencia inicial aleatoria del conjunto de datos de entrenamiento. Esta secuencia servirá como \"semilla\" para iniciar el proceso de generación de texto.    \n",
    "La idea es proporcionar al modelo un contexto inicial basado en el cual él puede comenzar a generar el siguiente carácter.   \n",
    "\n",
    "- **start** es un índice aleatorio que determina el punto de partida en el conjunto de datos.\n",
    "- **pattern** es la secuencia de caracteres (en forma de índices numéricos) extraída de *dataX* usando el índice *start*.\n",
    "\n",
    "A continuación, se imprime esta secuencia convertida de nuevo a caracteres para visualizar la semilla con la que se inicia la generación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5591db5e262d45e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T10:53:00.357810Z",
     "start_time": "2024-05-17T10:53:00.353697Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semilla:\n",
      "\" un lu \"\n"
     ]
    }
   ],
   "source": [
    "# Demostrar la generación de texto\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Semilla:\")\n",
    "print(\"\\\"\", ''.join([chars[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a3fc63ca3c5310",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Generación de caracteres\n",
    "\n",
    "El código arranca un bucle donde, en cada iteración, se intenta predecir el siguiente carácter basado en la secuencia actual (**pattern**).   \n",
    "\n",
    "Seguidamente se actualiza la secuencia moviéndola un paso hacia adelante, incluyendo el nuevo carácter predicho al final y descartando el primer carácter para mantener la longitud constante.   \n",
    "\n",
    "- **x** reformatea la secuencia *pattern* para que tenga el formato adecuado para el modelo: [1, longitud de la secuencia, 1], y la normaliza dividiendo por el número de caracteres únicos.\n",
    "- **prediction** es el vector de salida del modelo, donde cada elemento representa la probabilidad de que un carácter sea el siguiente en la secuencia.\n",
    "- **index** es el índice del carácter con la mayor probabilidad en prediction.\n",
    "- **result** es el carácter correspondiente al índice index, que se añade a la secuencia generada y se imprime.\n",
    "- La secuencia **pattern** se actualiza añadiendo el índice del carácter predicho al final y eliminando el primer elemento para mantener el tamaño.   \n",
    "\n",
    "Este bucle permite visualizar cómo el modelo, basándose en una semilla inicial y su \"memoria\" de lo aprendido durante el entrenamiento, puede generar texto que sigue alguna forma de estructura gramatical y léxica, dependiendo de la complejidad del texto original y la duración del entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f56c223aeab072a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T10:53:06.529781Z",
     "start_time": "2024-05-17T10:53:03.668833Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dduadeo he da maychnu rco r  derdduadeo he da mayc\n",
      "Terminado.\n"
     ]
    }
   ],
   "source": [
    "# Generar caracteres\n",
    "for i in range(50):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(len(chars))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = chars[index]\n",
    "    seq_in = [chars[value] for value in pattern]\n",
    "    print(result, end='')\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nTerminado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82626bdca93a7bdf",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

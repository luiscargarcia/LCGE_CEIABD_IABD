{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 15:37:40.304921: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742830660.328029   35951 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742830660.335267   35951 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-24 15:37:40.361842: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SCALED DOT-PRODUCT ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Clase 1: Scaled Dot-Product Attention\n",
    "# -----------------------------------------------------------------------------\n",
    "# Esta clase implementa el cálculo básico de la \"atención\" (attention).\n",
    "# Recibe tres tensores: Q (query), K (key) y V (value). \n",
    "# El mecanismo:\n",
    "#   - Se calcula la similitud (producto punto) entre Q y K.\n",
    "#   - Se escala dividiendo entre la raíz de la dimensión (d_k).\n",
    "#   - Se aplica softmax para obtener coeficientes de importancia (atención).\n",
    "#   - Se multiplica esos coeficientes por V para obtener la salida.\n",
    "# -----------------------------------------------------------------------------\n",
    "class ScaledDotProductAttention(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ScaledDotProductAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, q, k, v, mask=None):\n",
    "        # q, k, v tienen forma (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "        # 1) Producto punto Q*K^T para obtener la matriz de atención\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "        \n",
    "        # d_k = depth (dimensión de k)\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        # 2) Escalado por raíz de d_k\n",
    "        scaled_attention_logits = matmul_qk / tf.sqrt(dk)\n",
    "\n",
    "        # Opcional: Se puede aplicar máscara (por ejemplo en el decodificador).\n",
    "        if mask is not None:\n",
    "            # Máscara se suele aplicar sumando un valor muy negativo,\n",
    "            # para anular esos valores tras el softmax.\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "        # 3) Softmax para convertir logits en coeficientes de atención\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "        # 4) Multiplicamos los pesos de atención por V\n",
    "        output = tf.matmul(attention_weights, v)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MULTI-HEAD ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Clase 2: Multi-Head Attention\n",
    "# -----------------------------------------------------------------------------\n",
    "# El transformador divide la atención en varios \"cabezas\" (heads).\n",
    "# Se hace una proyección lineal de Q, K y V para cada cabeza, se aplica\n",
    "# atención (ScaledDotProductAttention) de forma independiente, y luego\n",
    "# se concatena el resultado para devolverlo a la dimensión original.\n",
    "# -----------------------------------------------------------------------------\n",
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Verificamos que d_model sea divisible por num_heads\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        # Capas densas para proyectar Q, K y V\n",
    "        self.wq = layers.Dense(d_model)\n",
    "        self.wk = layers.Dense(d_model)\n",
    "        self.wv = layers.Dense(d_model)\n",
    "\n",
    "        # Capa final para la proyección tras la concatenación\n",
    "        self.dense = layers.Dense(d_model)\n",
    "        \n",
    "        # Clase para la atención con producto punto escalado\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Divide la última dimensión en (num_heads, depth).\n",
    "        Reordena la salida a (batch_size, num_heads, seq_len, depth).\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])  # para que num_heads quede en la segunda posición\n",
    "\n",
    "    def call(self, v, k, q, mask=None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        # Proyectamos Q, K y V\n",
    "        q = self.wq(q)  # (batch_size, seq_len_q, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len_k, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len_v, d_model)\n",
    "\n",
    "        # Dividimos cada una en num_heads\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # Aplicamos atención escalada\n",
    "        scaled_attention, attention_weights = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        # Reordenamos de vuelta y concatenamos\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        # Proyección final\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CAPA FEED-FORWARD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Clase 3: Feed Forward (Positionalwise Feed-Forward Network)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Cada \"bloque\" del transformador tiene una red densa de dos capas.\n",
    "# La primera expande la dimensionalidad (ff_dim) y la segunda la reduce de nuevo.\n",
    "# -----------------------------------------------------------------------------\n",
    "class PositionwiseFeedForward(layers.Layer):\n",
    "    def __init__(self, d_model, ff_dim, **kwargs):\n",
    "        super(PositionwiseFeedForward, self).__init__(**kwargs)\n",
    "        self.dense1 = layers.Dense(ff_dim, activation='relu')\n",
    "        self.dense2 = layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. POSITIONAL ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Clase 4: Positional Encoding\n",
    "# -----------------------------------------------------------------------------\n",
    "# El transformador no usa convoluciones ni recurencia, así que para\n",
    "# que la red \"entienda\" la posición de cada token, se inyecta información\n",
    "# posicional mediante senos y cosenos.\n",
    "# -----------------------------------------------------------------------------\n",
    "def positional_encoding(position, d_model):\n",
    "    \"\"\"\n",
    "    Genera un tensor de tamaño (position, d_model) con el encoding posicional.\n",
    "    \"\"\"\n",
    "    # Generamos las posiciones (0, 1, 2, ..., position-1)\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)\n",
    "\n",
    "    # Se aplican seno a las posiciones pares de la dimensión de modelo\n",
    "    # y coseno a las posiciones impares\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "    cosines = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = np.zeros(angle_rads.shape)\n",
    "    pos_encoding[:, 0::2] = sines\n",
    "    pos_encoding[:, 1::2] = cosines\n",
    "\n",
    "    # Devolvemos pos_encoding con una dimensión extra para batch\n",
    "    return tf.cast(pos_encoding[np.newaxis, ...], dtype=tf.float32)\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    \"\"\"\n",
    "    Función auxiliar para calcular los ángulos para el encoding posicional.\n",
    "    \"\"\"\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CAPA DE ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Clase 5: Capa de Encoder\n",
    "# -----------------------------------------------------------------------------\n",
    "# Cada capa del encoder consiste en:\n",
    "#   1. Multi-head attention (con residual connection + layer normalization).\n",
    "#   2. Feed Forward (de dos capas, con residual connection + layer normalization).\n",
    "# -----------------------------------------------------------------------------\n",
    "class EncoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, dropout_rate=0.1, **kwargs):\n",
    "        super(EncoderLayer, self).__init__(**kwargs)\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, ff_dim)\n",
    "\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, mask=None, training=False):\n",
    "        # Multi-head attention\n",
    "        attn_output, _ = self.mha(x, x, x, mask=mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        # Feed Forward\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. CLASE ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Clase 6: Encoder\n",
    "# -----------------------------------------------------------------------------\n",
    "# El encoder está compuesto por:\n",
    "#   - Embedding + Positional Encoding\n",
    "#   - N capas de EncoderLayer\n",
    "# -----------------------------------------------------------------------------\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, ff_dim,\n",
    "                 input_vocab_size, maximum_position_encoding, dropout_rate=0.1, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Capa de embedding (para tokens)\n",
    "        self.embedding = layers.Embedding(input_vocab_size, d_model)\n",
    "\n",
    "        # Cálculo de la codificación posicional\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model, num_heads, ff_dim, dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, mask=None, training=False):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # Sumar embedding y codificación posicional\n",
    "        x = self.embedding(x)  # (batch_size, seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, mask=mask, training=training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. CAPA DE DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Clase 7: Capa de Decoder\n",
    "# -----------------------------------------------------------------------------\n",
    "# Cada capa del decoder consiste en:\n",
    "#   1. Masked multi-head attention (para que el decoder no vea \"futuro\").\n",
    "#   2. Multi-head attention recibiendo la salida del encoder.\n",
    "#   3. Feed Forward.\n",
    "# Cada bloque con sus conexiones residuales y normalización de capa.\n",
    "# -----------------------------------------------------------------------------\n",
    "class DecoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, dropout_rate=0.1, **kwargs):\n",
    "        super(DecoderLayer, self).__init__(**kwargs)\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)  # masked MHA\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)  # MHA con salida del encoder\n",
    "\n",
    "        self.ffn = PositionwiseFeedForward(d_model, ff_dim)\n",
    "\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "        self.dropout3 = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, enc_output, look_ahead_mask=None, padding_mask=None, training=False):\n",
    "        # 1) Masked multi-head attention\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, mask=look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(x + attn1)\n",
    "\n",
    "        # 2) Multi-head attention con la salida del encoder\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, mask=padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(out1 + attn2)\n",
    "\n",
    "        # 3) Feed Forward\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(out2 + ffn_output)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. CLASE DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Clase 8: Decoder\n",
    "# -----------------------------------------------------------------------------\n",
    "# El decoder está compuesto por:\n",
    "#   - Embedding + Positional Encoding\n",
    "#   - N capas de DecoderLayer\n",
    "# -----------------------------------------------------------------------------\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, ff_dim,\n",
    "                 target_vocab_size, maximum_position_encoding, dropout_rate=0.1, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Embedding para la secuencia de salida\n",
    "        self.embedding = layers.Embedding(target_vocab_size, d_model)\n",
    "\n",
    "        # Codificación posicional\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [\n",
    "            DecoderLayer(d_model, num_heads, ff_dim, dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, enc_output, look_ahead_mask=None, padding_mask=None, training=False):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # Sumar embedding y codificación posicional\n",
    "        x = self.embedding(x)  # (batch_size, seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        attention_weights = {}\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x,\n",
    "                                                   enc_output,\n",
    "                                                   look_ahead_mask=look_ahead_mask,\n",
    "                                                   padding_mask=padding_mask,\n",
    "                                                   training=training)\n",
    "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "\n",
    "        return x, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. TRANSFORMER COMPLETO (ENCODER+DECODER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Clase 9: Transformer completo (Encoder + Decoder)\n",
    "# -----------------------------------------------------------------------------\n",
    "class Transformer(keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, ff_dim,\n",
    "                 input_vocab_size, target_vocab_size, \n",
    "                 pe_input, pe_target, dropout_rate=0.1, **kwargs):\n",
    "        super(Transformer, self).__init__(**kwargs)\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, ff_dim,\n",
    "                               input_vocab_size, pe_input, dropout_rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, ff_dim,\n",
    "                               target_vocab_size, pe_target, dropout_rate)\n",
    "\n",
    "        self.final_layer = layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs, training=False, enc_padding_mask=None,\n",
    "             look_ahead_mask=None, dec_padding_mask=None):\n",
    "        \"\"\"\n",
    "        inputs: tupla (inp, tar)\n",
    "            inp -> secuencia de entrada (batch, seq_len_in)\n",
    "            tar -> secuencia de salida (batch, seq_len_out)\n",
    "        \"\"\"\n",
    "\n",
    "        inp, tar = inputs\n",
    "        # Salida del encoder\n",
    "        enc_output = self.encoder(inp, mask=enc_padding_mask, training=training)\n",
    "\n",
    "        # Salida del decoder\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output,\n",
    "            look_ahead_mask=look_ahead_mask,\n",
    "            padding_mask=dec_padding_mask,\n",
    "            training=training\n",
    "        )\n",
    "        \n",
    "        # Guardamos los pesos de atención en un atributo, \n",
    "        # pero no los retornamos como salida \"oficial\" del modelo\n",
    "        self._attention_weights = attention_weights\n",
    "\n",
    "        # Capa final densa para predecir el siguiente token\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, seq_len_out, vocab_size)\n",
    "\n",
    "        return final_output\n",
    "    \n",
    "    def get_attention_weights(self):\n",
    "        \"\"\"Método auxiliar para acceder a los pesos de atención si se desea.\"\"\"\n",
    "        return self._attention_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. EJEMPLO DE EJECUCIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EJEMPLO DE USO\n",
    "# -----------------------------------------------------------------------------\n",
    "# A continuación construimos un pequeño Transformer para mostrar cómo se usa.\n",
    "# No se entrena con datos reales, simplemente ilustra la compilación y llamada.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parámetros\n",
    "    num_layers = 2       # número de capas en encoder/decoder\n",
    "    d_model = 128        # dimensión de embeddings\n",
    "    num_heads = 4        # número de cabezas en Multi-Head Attention\n",
    "    ff_dim = 512         # dimensión interna de la red feed-forward\n",
    "    input_vocab_size = 8500\n",
    "    target_vocab_size = 8000\n",
    "    dropout_rate = 0.1\n",
    "\n",
    "    # Creamos un Transformer\n",
    "    transformer = Transformer(\n",
    "        num_layers=num_layers,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        ff_dim=ff_dim,\n",
    "        input_vocab_size=input_vocab_size,\n",
    "        target_vocab_size=target_vocab_size,\n",
    "        pe_input=10000,   # máximo de posiciones en la entrada\n",
    "        pe_target=6000,   # máximo de posiciones en la salida\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "\n",
    "    # Compilamos el modelo con optimizador y pérdida\n",
    "    # (Por ejemplo, la entropía cruzada categórica si se tratara de un problema de clasificación de tokens)\n",
    "    transformer.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Generamos datos de ejemplo (batch_size=2, secuencia de 5 tokens)\n",
    "    dummy_inp = tf.constant([[1,2,3,4,5],[4,5,6,7,0]])\n",
    "    dummy_tar = tf.constant([[1,2,3,4,5],[2,3,4,5,6]])\n",
    "\n",
    "    # Llamamos al modelo\n",
    "    # normalmentte deberíamos construir máscaras, pero para un ejemplo sencillo las omitimos\n",
    "    pred, attn = transformer((dummy_inp, dummy_tar), training=False)\n",
    "\n",
    "    print(\"Forma de la salida:\", pred.shape)  # (batch_size, seq_len_out, target_vocab_size)\n",
    "    # Por ejemplo -> (2, 5, 8000)\n",
    "\n",
    "    # Probamos un paso de entrenamiento \"dummy\"\n",
    "    # Para entrenamiento real, se necesitan datos reales y máscaras adecuadas\n",
    "    history = transformer.fit(x=(dummy_inp, dummy_tar),\n",
    "                              y=dummy_tar,  # normalmente: y son los mismos tar desplazados\n",
    "                              epochs=1,\n",
    "                              verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EJEMPLO DE USO CON DATASET SENCILLO REAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario ES: ['<pad>', '<start>', '<end>', 'adios', 'bien', 'buenas', 'buenos', 'como', 'dias', 'estas', 'gracias', 'hola', 'mundo', 'muy', 'noches']\n",
      "Vocabulario EN: ['<pad>', '<start>', '<end>', 'are', 'bye', 'good', 'hello', 'how', 'morning', 'night', 'thanks', 'very', 'world']\n",
      "Tamaño vocab ES: 15\n",
      "Tamaño vocab EN: 13\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 15:38:44.530098: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "Exception encountered when calling Transformer.call().\n\n\u001b[1mIterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\u001b[0m\n\nArguments received by Transformer.call():\n  • inputs=tf.Tensor(shape=(None, 4), dtype=int32)\n  • training=True\n  • enc_padding_mask=None\n  • look_ahead_mask=None\n  • dec_padding_mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 125\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# La entrada X será (es_batch, en_batch) y la \"y\" será en_batch *desplazado*.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Sin embargo, aquí simplificamos y usamos en_batch como \"y\" directamente.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# (En un training real, habría que desplazar una posición la secuencia target).\u001b[39;00m\n\u001b[1;32m    124\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m--> 125\u001b[0m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# 2.7) Ejemplo de inferencia simple\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Vamos a traducir la frase \"buenos dias\" -> \"???\" con el modelo\u001b[39;00m\n\u001b[1;32m    134\u001b[0m test_es \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuenos dias\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[13], line 26\u001b[0m, in \u001b[0;36mTransformer.call\u001b[0;34m(self, inputs, training, enc_padding_mask, look_ahead_mask, dec_padding_mask)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, enc_padding_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m          look_ahead_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dec_padding_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     20\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    inputs: tupla (inp, tar)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m        inp -> secuencia de entrada (batch, seq_len_in)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m        tar -> secuencia de salida (batch, seq_len_out)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     inp, tar \u001b[38;5;241m=\u001b[39m inputs\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Salida del encoder\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     enc_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(inp, mask\u001b[38;5;241m=\u001b[39menc_padding_mask, training\u001b[38;5;241m=\u001b[39mtraining)\n",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: Exception encountered when calling Transformer.call().\n\n\u001b[1mIterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\u001b[0m\n\nArguments received by Transformer.call():\n  • inputs=tf.Tensor(shape=(None, 4), dtype=int32)\n  • training=True\n  • enc_padding_mask=None\n  • look_ahead_mask=None\n  • dec_padding_mask=None"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# NECESARIO USO DE CUDA Y GPU PARA EJECUTAR ESTE CÓDIGO DE EJEMPLO\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Ejemplo con dataset real (simple) para traducción ES -> EN\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ---------------------------------------------------------\n",
    "    # 2.1) Creamos un mini dataset manual (Es -> En)\n",
    "    # ---------------------------------------------------------\n",
    "    # Cada item es (input_sentence, target_sentence)\n",
    "    pairs = [\n",
    "        (\"hola\",             \"hello\"),\n",
    "        (\"adios\",            \"bye\"),\n",
    "        (\"buenos dias\",      \"good morning\"),\n",
    "        (\"buenas noches\",    \"good night\"),\n",
    "        (\"como estas\",       \"how are\"),\n",
    "        (\"muy bien\",         \"very good\"),\n",
    "        (\"gracias\",          \"thanks\"),\n",
    "        (\"mundo\",            \"world\"),\n",
    "    ]\n",
    "\n",
    "    # En problemas reales se usan tokenizadores más sofisticados. Aquí haremos\n",
    "    # uno básico basado en separar por espacios y mapear palabra->ID.\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2.2) Construimos el vocabulario (palabras -> IDs)\n",
    "    # ---------------------------------------------------------\n",
    "    # Extraemos todas las palabras de ES e inglés\n",
    "    es_vocab = set()\n",
    "    en_vocab = set()\n",
    "    for es, en in pairs:\n",
    "        for w in es.split():\n",
    "            es_vocab.add(w)\n",
    "        for w in en.split():\n",
    "            en_vocab.add(w)\n",
    "\n",
    "    # Añadimos tokens especiales\n",
    "    # <pad> (0), <start> (1), <end> (2)\n",
    "    es_index2word = [\"<pad>\", \"<start>\", \"<end>\"] + sorted(list(es_vocab))\n",
    "    en_index2word = [\"<pad>\", \"<start>\", \"<end>\"] + sorted(list(en_vocab))\n",
    "\n",
    "    es_word2index = {w: i for i, w in enumerate(es_index2word)}\n",
    "    en_word2index = {w: i for i, w in enumerate(en_index2word)}\n",
    "\n",
    "    es_vocab_size = len(es_index2word)\n",
    "    en_vocab_size = len(en_index2word)\n",
    "\n",
    "    print(\"Vocabulario ES:\", es_index2word)\n",
    "    print(\"Vocabulario EN:\", en_index2word)\n",
    "    print(\"Tamaño vocab ES:\", es_vocab_size)\n",
    "    print(\"Tamaño vocab EN:\", en_vocab_size)\n",
    "\n",
    "     # ---------------------------------------------------------\n",
    "    # 2.3) Convertimos las frases a listas de IDs\n",
    "    # ---------------------------------------------------------\n",
    "    def encode_es_sentence(sentence):\n",
    "        # <start> tokens + IDs + <end> tokens\n",
    "        words = sentence.split()\n",
    "        return [es_word2index[\"<start>\"]] + [es_word2index[w] for w in words] + [es_word2index[\"<end>\"]]\n",
    "\n",
    "    def encode_en_sentence(sentence):\n",
    "        # <start> tokens + IDs + <end> tokens\n",
    "        words = sentence.split()\n",
    "        return [en_word2index[\"<start>\"]] + [en_word2index[w] for w in words] + [en_word2index[\"<end>\"]]\n",
    "\n",
    "    es_sequences = []\n",
    "    en_sequences = []\n",
    "    for es, en in pairs:\n",
    "        es_sequences.append(encode_es_sentence(es))\n",
    "        en_sequences.append(encode_en_sentence(en))\n",
    "\n",
    "    # Hacemos padding a la misma longitud\n",
    "    # Buscamos la secuencia más larga en ES y EN\n",
    "    max_len_es = max(len(seq) for seq in es_sequences)\n",
    "    max_len_en = max(len(seq) for seq in en_sequences)\n",
    "\n",
    "    es_padded = [seq + [0]*(max_len_es - len(seq)) for seq in es_sequences]\n",
    "    en_padded = [seq + [0]*(max_len_en - len(seq)) for seq in en_sequences]\n",
    "\n",
    "    es_padded = np.array(es_padded, dtype=np.int32)\n",
    "    en_padded = np.array(en_padded, dtype=np.int32)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2.4) Construimos un Dataset de TensorFlow (batch pequeño)\n",
    "    # ---------------------------------------------------------\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((es_padded, en_padded))\n",
    "    dataset = dataset.shuffle(buffer_size=8).batch(2)  # batch=2 para ejemplo\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2.5) Instanciamos el Transformer\n",
    "    # ---------------------------------------------------------\n",
    "    num_layers = 2\n",
    "    d_model = 64\n",
    "    num_heads = 2\n",
    "    ff_dim = 128\n",
    "    dropout_rate = 0.1\n",
    "\n",
    "    transformer = Transformer(\n",
    "        num_layers=num_layers,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        ff_dim=ff_dim,\n",
    "        input_vocab_size=es_vocab_size,\n",
    "        target_vocab_size=en_vocab_size,\n",
    "        pe_input=100,   # máximo de posiciones en la entrada\n",
    "        pe_target=100,  # máximo de posiciones en la salida\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2.6) Compilamos y entrenamos\n",
    "    # ---------------------------------------------------------\n",
    "    # Usamos entropía cruzada categórica (sparce) para secuencias, desde logits.\n",
    "    # La métrica 'accuracy' no suele ser tan ilustrativa para traducción,\n",
    "    # pero la dejamos para ver si en algo mejora en un toy example.\n",
    "    transformer.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # La entrada X será (es_batch, en_batch) y la \"y\" será en_batch *desplazado*.\n",
    "    # Sin embargo, aquí simplificamos y usamos en_batch como \"y\" directamente.\n",
    "    # (En un training real, habría que desplazar una posición la secuencia target).\n",
    "    EPOCHS = 10\n",
    "    transformer.fit(\n",
    "        dataset,\n",
    "        epochs=EPOCHS\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2.7) Ejemplo de inferencia simple\n",
    "    # ---------------------------------------------------------\n",
    "    # Vamos a traducir la frase \"buenos dias\" -> \"???\" con el modelo\n",
    "    test_es = \"buenos dias\"\n",
    "    test_es_seq = encode_es_sentence(test_es)\n",
    "    # Lo convertimos en un batch de tamaño 1, con padding\n",
    "    test_es_seq_padded = test_es_seq + [0]*(max_len_es - len(test_es_seq))\n",
    "    test_es_seq_padded = np.array([test_es_seq_padded])  # shape (1, max_len_es)\n",
    "\n",
    "    # \"Decodificar\" token a token sería lo ideal. Para este ejemplo, \n",
    "    # haremos un \"inference rápido\" alimentando la secuencia entera:\n",
    "    test_en_seq_in = [en_word2index[\"<start>\"]]\n",
    "    # generamos \"a ciegas\" hasta longitud máxima\n",
    "    max_steps = max_len_en\n",
    "    predicted_words = []\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        # Empaquetamos la secuencia generada hasta el momento en un batch\n",
    "        test_en_padded = test_en_seq_in + [0]*(max_len_en - len(test_en_seq_in))\n",
    "        test_en_padded = np.array([test_en_padded])\n",
    "\n",
    "        # Pasamos (input_es, input_en) al transformer\n",
    "        logits = transformer((test_es_seq_padded, test_en_padded), training=False)\n",
    "        # logits shape: (1, max_len_en, en_vocab_size)\n",
    "        next_token_logits = logits[0, len(test_en_seq_in)-1, :]  # último paso\n",
    "        next_token_id = int(tf.argmax(next_token_logits, axis=-1).numpy())\n",
    "\n",
    "        # Rompemos si es <end>\n",
    "        if next_token_id == en_word2index[\"<end>\"]:\n",
    "            break\n",
    "\n",
    "        predicted_words.append(en_index2word[next_token_id])\n",
    "        test_en_seq_in.append(next_token_id)\n",
    "\n",
    "    predicted_sentence = \" \".join(predicted_words)\n",
    "    print(f\"Traducción de '{test_es}' => '{predicted_sentence}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***LOGITS***\n",
    "Los *logits* son las salidas sin normalizar de la red. Se suelen usar en combinación con la función de pérdida de entropía cruzada, que internamente aplica la operación necesaria (softmax + cálculo de entropía cruzada) de la forma más eficiente y estable.   \n",
    "Cuando una red neuronal realiza una clasificación (imaginemos que queremos clasificar entre varias clases), la última capa suele producir un vector de dimensión igual al número de clases. A este vector se le llaman “logits”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULTADO DE EJECUCIÓN DEL TRANSFORMER\n",
    "**Forma de la salida: (5, 8000)**:\n",
    "\n",
    "El modelo está produciendo un tensor cuyas dimensiones son (sequence_length, vocab_size).\n",
    "En este caso, la secuencia de salida tiene 5 posiciones (tokens) y el vocabulario tiene 8000 posibles tokens diferentes.\n",
    "\n",
    "Si se dispone de un batch size de 1 (una sola secuencia de entrada/salida en este ejemplo), a veces TensorFlow \"colapsa\" la dimensión de batch y acaba mostrándo solo (5, 8000).\n",
    "Si se tiene un batch_size mayor a 1, típicamente se vería algo como (batch_size, seq_length, vocab_size).\n",
    "\n",
    "Indica que se está ejecutando una sola iteración de entrenamiento (un minibatch) durante la epoch (puesto que solo se ha proporcionado un conjunto de datos muy pequeño, de tamaño 1).\n",
    "\n",
    "\n",
    "**La precisión (accuracy) es 0.0.**\n",
    "\n",
    "Esto es normal en un solo paso de entrenamiento con datos aleatorios y pesos aleatorios. Si el modelo no acierta ninguno de los tokens, la exactitud para ese batch es 0.\n",
    "\n",
    "**loss: 9.0551:**\n",
    "\n",
    "Es el valor de la función de pérdida (en este caso, entropía cruzada).\n",
    "Un valor alrededor de 9 es típico cuando se genera una secuencia de logits muy dispersos sobre 8000 posibles tokens, sin haber realizado un entrenamiento real.\n",
    "\n",
    "En resumen, el mensaje de salida está indicando:\n",
    "\n",
    "- Qué forma tiene la salida de el Transformer (la capa final produce logits de dimensión [seq_len, vocab_size] o [batch_size, seq_len, vocab_size], si el batch_size no se colapsa).\n",
    "\n",
    "- Qué ocurrió en ese único paso de entrenamiento de ejemplo: la exactitud fue 0 y la pérdida aproximadamente 9.05, algo esperado al no haber entrenado con datos reales ni por varias épocas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

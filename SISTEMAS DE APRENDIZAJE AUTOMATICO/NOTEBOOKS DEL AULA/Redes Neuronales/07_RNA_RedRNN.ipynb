{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97b3311348783022",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## RED NEURONAL RECURRENTE - (*RNN - Recurrent Neural Network o Red de ELMAN*)   \n",
    "\n",
    "Las redes de Elman son el modelo más simple de Red Neuronal Recurrente (RNN). Tienen la misma estructura que las redes neuronales vistas hasta ahora, salvo por una única circunstancia: se permite que cada neurona se **retroalimente a sí misma**.\n",
    "\n",
    "<img src=\"./img/Elman.jpg\">   \n",
    "\n",
    "Donde $h_t$ es el estado de la neurona en el momento $t$, $h_{t-1}$ su estado en el momento inmediatamente anterior; $w_i$ representa los pesos sinápticos, $d_i$ los valores de activación de las neuronas de la capa anterior y $b$ el sesgo(NO APARECE EN LA IMAGEN). Como se puede observar, existe un término extra $Uh_{t-1}$ que no existe en el caso de las redes no recurrentes, y que en este caso permite que cada neurona se excite a sí misma.    \n",
    "\n",
    "Como se puede ver, por tanto, en las neuronas del modelo de Elman se forma un pequeño bucle de retroalimentación, mediante el cual el axón de la neurona excita una de sus propias dendritas. El peso sináptico de esta conexión de retroalimentación es $U$.\n",
    "\n",
    "El valor de activación en el momento anterior $h_{t-1}$ contribuye a la suma total de excitaciones de la célula $(x)$, a través del coeficiente $U$. Este coeficiente funcionará de forma equivalente a un peso sináptico.\n",
    "\n",
    "## EJEMPLO DE RNN\n",
    "\n",
    "\n",
    "Este es un ejemplo sencillo de una RNN que va a permitir llevar a cabo la predicción de caracteres usando un conjunto de datos (cadena de texto)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f733a16eae5534a8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### IMPORTAR LIBRERÍAS NECESARIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc002759",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T09:34:13.029545Z",
     "start_time": "2024-05-17T09:34:13.026310Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 21:49:41.284164: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-19 21:49:41.338785: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742420981.409639   45652 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742420981.429440   45652 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-19 21:49:41.502000: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a04e47483b754f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Desactivamos los WARNINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb20459109210b9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T09:34:14.942700Z",
     "start_time": "2024-05-17T09:34:14.939567Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f79c3e366674f0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### DEFINICIÓN DEL CONJUNTO DE DATOS Y CREACIÓN DEL VOCABULARIO.\n",
    "\n",
    "Generamos el conjunto de datos a partir de una frase sencilla y, seguidamente, creamos un vocabulario ordenando el conjunto de carácteres que componen la frase.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "883636883fd4ed10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T09:34:18.091538Z",
     "start_time": "2024-05-17T09:34:18.087407Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario:  [' ', 'a', 'd', 'h', 'l', 'm', 'n', 'o', 'u']\n",
      "Tamaño del vocabulario:  9\n"
     ]
    }
   ],
   "source": [
    "# Definimos el conjunto de datos (secuencia de texto)\n",
    "text = \"hola mundo\"\n",
    "# Crear un vocabulario de caracteres\n",
    "vocab = sorted(set(text))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"Vocabulario: \", vocab)\n",
    "print(\"Tamaño del vocabulario: \", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dce2f85",
   "metadata": {},
   "source": [
    "\n",
    "Seguidamente se crea un diccionario mapeando los caracteres del vocabulario con índices para, a continuación, transformar la secuencia de caracteres que conforman la frase en una secuencia de indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1a3450c785eb3e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T09:34:19.322169Z",
     "start_time": "2024-05-17T09:34:19.317533Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapeo de caracteres a índices:  {' ': 0, 'a': 1, 'd': 2, 'h': 3, 'l': 4, 'm': 5, 'n': 6, 'o': 7, 'u': 8}\n",
      "Mapeo de índices a caracteres:  [' ' 'a' 'd' 'h' 'l' 'm' 'n' 'o' 'u']\n",
      "Texto como índices:  [3 7 4 1 0 5 8 6 2 7]\n"
     ]
    }
   ],
   "source": [
    "# Crear un diccionario de mapeo de caracteres a índices\n",
    "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx_to_char = np.array(vocab)\n",
    "print(\"Mapeo de caracteres a índices: \", char_to_idx)\n",
    "print(\"Mapeo de índices a caracteres: \", idx_to_char)\n",
    "\n",
    "# Convertir la secuencia de texto a una secuencia de índices\n",
    "text_as_int = np.array([char_to_idx[c] for c in text])\n",
    "print(\"Texto como índices: \", text_as_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbec2251c95c7a56",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### PREPARACIÓN DE DATOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592ca282",
   "metadata": {},
   "source": [
    "Para preparar los datos, en el siguiente bloque de código se establece la longitud de la secuencia de entrada (también denominada \"ventana\") a un valor de 4 caracteres, por ejemplo: \"mund\" -> \"o\".      \n",
    "\n",
    "En este caso, para el entrenamiento se generan un total de 6 ejemplos para la secuencia de partida: \"hola mundo\".    \n",
    "\n",
    "Finalizado este proceso, se dispondrá de los datos de entrenamiento formando un conjunto de pares de secuencia de entrada y salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "895f3836ec9abc8b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preparar los datos de entrenamiento y etiquetas\n",
    "seq_length = 4 # Tamaño de la ventana = 4. Se toman 4 caracteres como entrada y se predice el siguiente caracter\n",
    "examples_per_epoch = len(text) - seq_length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cab6007b552526c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T09:34:20.511898Z",
     "start_time": "2024-05-17T09:34:20.504071Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Crear las secuencias de entrada y salida\n",
    "inputs = np.array([text_as_int[i:i+seq_length] for i in range(examples_per_epoch)])\n",
    "targets = np.array([text_as_int[i+seq_length] for i in range(examples_per_epoch)])\n",
    "\n",
    "# Reshape para cumplir con el formato esperado por la RNN\n",
    "inputs = np.reshape(inputs, (examples_per_epoch, seq_length, 1)) # se corresponde con (batch_size, seq_length, input_dim)\n",
    "\n",
    "# Usar tf.data.Dataset para manejar los datos\n",
    "# y crear un dataset a partir de los tensores de entrada y salida (inputs, targets)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)) #from_tensor_slices crea un dataset a partir de tensores, donde cada tensor es una muestra de entrada y salida\n",
    "# batch(1) indica que se toma un solo ejemplo por batch y drop_remainder=True indica que se descartan los ejemplos que no se ajustan al tamaño del batch\n",
    "dataset = dataset.batch(1, drop_remainder=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93a1f4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplos por época:  6\n",
      "Entradas:  (6, 4, 1)\n",
      "Etiquetas:  (6,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Ejemplos por época: \", examples_per_epoch)\n",
    "print(\"Entradas: \", inputs.shape)\n",
    "print(\"Etiquetas: \", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffa907a73440f33",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### DISEÑO DE LA ARQUITECTURA DE LA RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0abd5c906b27a21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T09:34:22.194283Z",
     "start_time": "2024-05-17T09:34:22.161885Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Crear el modelo RNN\n",
    "model = Sequential([\n",
    "    SimpleRNN(50,                           # Número de unidades en la capa oculta (50)\n",
    "              input_shape=(seq_length, 1),  # Tamaño de la ventana y número de características (1)\n",
    "              return_sequences=False),      #return_sequences=False indica que solo se devuelve la salida de la última capa\n",
    "    \n",
    "    Dense(vocab_size,                       # Número de unidades en la capa de salida (vocab_size, tamaño del vocabulario)\n",
    "          activation='softmax')             # Función de activación de la capa de salida (softmax), ya que se trata de un problema de clasificación multiclase.\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec9558c",
   "metadata": {},
   "source": [
    "### COMPILACIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2451e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar el modelo\n",
    "model.compile(optimizer=Adam(),                         # Optimizador Adam por defecto\n",
    "              loss='sparse_categorical_crossentropy')   #sparse_categorical_crossentropy ya que se trata de un problema de clasificación multiclase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9596470fd38681",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### ENTRENAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f722731b96e42ddf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T09:34:26.244997Z",
     "start_time": "2024-05-17T09:34:23.653061Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2.5336\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.1155 \n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.8852 \n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.7024 \n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.5499 \n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4195 \n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.3062 \n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.2069 \n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.1193 \n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.0418 \n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.9730 \n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.9117 \n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.8570 \n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.8081 \n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.7643 \n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.7249 \n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.6893 \n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.6569 \n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.6274 \n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.6004 \n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.5754 \n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5522 \n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5306 \n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5105 \n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4916 \n",
      "Epoch 26/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4740 \n",
      "Epoch 27/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4574 \n",
      "Epoch 28/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4418 \n",
      "Epoch 29/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4271 \n",
      "Epoch 30/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4131 \n",
      "Epoch 31/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3999 \n",
      "Epoch 32/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3874 \n",
      "Epoch 33/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3755 \n",
      "Epoch 34/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3641 \n",
      "Epoch 35/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3531\n",
      "Epoch 36/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3427 \n",
      "Epoch 37/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3327 \n",
      "Epoch 38/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3231 \n",
      "Epoch 39/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3138 \n",
      "Epoch 40/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3049 \n",
      "Epoch 41/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2963 \n",
      "Epoch 42/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2880 \n",
      "Epoch 43/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2800 \n",
      "Epoch 44/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2723 \n",
      "Epoch 45/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2648 \n",
      "Epoch 46/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2576 \n",
      "Epoch 47/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2506 \n",
      "Epoch 48/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2438 \n",
      "Epoch 49/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2372 \n",
      "Epoch 50/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2309 \n",
      "Epoch 51/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2247 \n",
      "Epoch 52/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2187 \n",
      "Epoch 53/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2129 \n",
      "Epoch 54/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2073 \n",
      "Epoch 55/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2018\n",
      "Epoch 56/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1965\n",
      "Epoch 57/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1914 \n",
      "Epoch 58/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1864 \n",
      "Epoch 59/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1815 \n",
      "Epoch 60/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1768 \n",
      "Epoch 61/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1722 \n",
      "Epoch 62/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1678 \n",
      "Epoch 63/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1634 \n",
      "Epoch 64/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1593 \n",
      "Epoch 65/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1552 \n",
      "Epoch 66/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1512 \n",
      "Epoch 67/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1474 \n",
      "Epoch 68/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1436 \n",
      "Epoch 69/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1400 \n",
      "Epoch 70/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1365 \n",
      "Epoch 71/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1331 \n",
      "Epoch 72/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1298 \n",
      "Epoch 73/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1265 \n",
      "Epoch 74/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1234 \n",
      "Epoch 75/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1204 \n",
      "Epoch 76/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1174 \n",
      "Epoch 77/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1146 \n",
      "Epoch 78/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1118 \n",
      "Epoch 79/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1091 \n",
      "Epoch 80/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1065 \n",
      "Epoch 81/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1039 \n",
      "Epoch 82/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1015 \n",
      "Epoch 83/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0991 \n",
      "Epoch 84/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0968 \n",
      "Epoch 85/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0945 \n",
      "Epoch 86/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0923 \n",
      "Epoch 87/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0902 \n",
      "Epoch 88/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0881 \n",
      "Epoch 89/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0861 \n",
      "Epoch 90/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0842 \n",
      "Epoch 91/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0823 \n",
      "Epoch 92/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0805 \n",
      "Epoch 93/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0787 \n",
      "Epoch 94/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0770 \n",
      "Epoch 95/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0753 \n",
      "Epoch 96/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0737 \n",
      "Epoch 97/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0721 \n",
      "Epoch 98/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0706 \n",
      "Epoch 99/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0691\n",
      "Epoch 100/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0677 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f55341486d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "model.fit(dataset, epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44be5ffc129d034",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### PREDICCIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "572b9e671594e410",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T09:35:14.271909Z",
     "start_time": "2024-05-17T09:35:14.096963Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Función para predecir el siguiente carácter en una secuencia dada\n",
    "def predict_next_char(model, input_text):\n",
    "    input_eval = np.array([char_to_idx[c] for c in input_text])         # Convertir la entrada a índices\n",
    "    input_eval = np.reshape(input_eval, (1, len(input_eval), 1))        # Reshape para cumplir con el formato esperado por la RNN\n",
    "    prediction = model.predict(input_eval)                              # Realizar la predicción\n",
    "    predicted_idx = np.argmax(prediction)                               # Obtener el índice del carácter predicho\n",
    "    return idx_to_char[predicted_idx]                                   # Obtener el carácter correspondiente al índice predicho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59be97ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step\n",
      "Entrada: 'mund' -> Siguiente carácter predicho: 'o'\n"
     ]
    }
   ],
   "source": [
    "# Probar el modelo\n",
    "input_text = \"mund\"\n",
    "predicted_char = predict_next_char(model, input_text)\n",
    "print(f\"Entrada: '{input_text}' -> Siguiente carácter predicho: '{predicted_char}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d664fe8c",
   "metadata": {},
   "source": [
    "## **NOTA SOBRE LAS RNN Y LAS TAREAS SOBRE SERIES TEMPORALES**   \n",
    "\n",
    "Las Redes Neuronales Recurrentes (RNN) se utilizan ampliamente para tareas con series temporales, aunque ciertas variantes son más adecuadas que las RNNs básicas.\n",
    "\n",
    "**¿Por qué las RNNs para series temporales?**\n",
    "\n",
    "* **Dependencia temporal:**\n",
    "    * Las series temporales son secuencias de datos donde el orden es crucial. Las RNNs están diseñadas para manejar datos secuenciales, ya que mantienen una \"memoria\" de las entradas anteriores. Esto les permite capturar dependencias temporales, es decir, cómo los valores pasados influyen en los valores futuros.\n",
    "    * Por ejemplo, para predecir el precio de una acción, es importante considerar los precios anteriores, ya que suelen existir patrones y tendencias a lo largo del tiempo.\n",
    "* **Procesamiento de secuencias de longitud variable:**\n",
    "    * Las RNNs pueden procesar secuencias de longitud variable, lo que es útil para series temporales que pueden tener diferentes duraciones.\n",
    "\n",
    "**Tipos de RNNs más utilizados para series temporales:**\n",
    "\n",
    "* **Redes LSTM (Long Short-Term Memory):**\n",
    "    * Las LSTM son una variante de las RNNs que abordan el problema del \"desvanecimiento del gradiente\", que dificulta el aprendizaje de dependencias a largo plazo.\n",
    "    * Las LSTM utilizan \"compuertas\" que controlan el flujo de información, lo que les permite recordar información relevante durante períodos prolongados.\n",
    "    * Son muy efectivas para series temporales con patrones complejos y dependencias a largo plazo, como la predicción del precio de acciones o el análisis del lenguaje natural.\n",
    "* **Redes GRU (Gated Recurrent Units):**\n",
    "    * Las GRU son una versión simplificada de las LSTM que también abordan el problema del desvanecimiento del gradiente.\n",
    "    * Tienen menos compuertas que las LSTM, lo que las hace computacionalmente más eficientes.\n",
    "    * Las GRU suelen ofrecer un rendimiento similar a las LSTM en muchas tareas de series temporales.\n",
    "\n",
    "**CONCLUSIÓN:**\n",
    "\n",
    "* Las RNNs, especialmente las LSTM y GRU, son herramientas poderosas para el análisis y la predicción de series temporales debido a su capacidad para capturar dependencias temporales.\n",
    "* Si bien las RNN basica se pueden usar, LSTM y GRU dan mejor rendimiento con series temporales.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SCALED DOT-PRODUCT ATTENTION   \n",
    "\n",
    "`ScaledDotProductAttention` es el mecanismo de atención central utilizado por el componente de atención multicabezal para calcular las puntuaciones de atención.    \n",
    "\n",
    "Calcula el producto punto entre las consultas y las claves, escala el resultado, aplica una máscara (si es necesario) y, a continuación, calcula la suma ponderada de los valores en función de los pesos de atención."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Clase 1: Scaled Dot-Product Attention\n",
    "# -----------------------------------------------------------------------------\n",
    "# Esta clase implementa el cálculo básico de la \"atención\" (attention).\n",
    "# Recibe tres tensores: Q (query), K (key) y V (value). \n",
    "# El mecanismo:\n",
    "#   - Se calcula la similitud (producto punto) entre Q y K.\n",
    "#   - Se escala dividiendo entre la raíz de la dimensión (d_k).\n",
    "#   - Se aplica softmax para obtener coeficientes de importancia (atención).\n",
    "#   - Se multiplica esos coeficientes por V para obtener la salida.\n",
    "# -----------------------------------------------------------------------------\n",
    "class ScaledDotProductAttention(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ScaledDotProductAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, q, k, v, mask=None):\n",
    "        # q, k, v tienen forma (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "        # 1) Producto punto Q*K^T para obtener la matriz de atención\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "        \n",
    "        # d_k = depth (dimensión de k)\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        # 2) Escalado por raíz de d_k\n",
    "        scaled_attention_logits = matmul_qk / tf.sqrt(dk)\n",
    "\n",
    "        # Opcional: Se puede aplicar máscara (por ejemplo en el decodificador).\n",
    "        if mask is not None:\n",
    "            # Máscara se suele aplicar sumando un valor muy negativo,\n",
    "            # para anular esos valores tras el softmax.\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "        # 3) Softmax para convertir logits en coeficientes de atención\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "        # 4) Multiplicamos los pesos de atención por V\n",
    "        output = tf.matmul(attention_weights, v)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MULTI-HEAD ATTENTION   \n",
    "\n",
    "El mecanismo de atención multicabezal (Multi-Head Attention) permite al modelo centrarse simultáneamente en distintas partes de la secuencia de entrada.    \n",
    "Utiliza varias cabezas de atención para calcular distintas representaciones de la entrada.\n",
    "\n",
    "- `Multi-Head Attention`: Esta clase realiza la atención multicabezal dividiendo la entrada en múltiples cabezales, lo que permite al modelo centrarse en diferentes partes de la secuencia simultáneamente.\n",
    "- `d_model` y `num_heads`: `d_model` es el tamaño del embedding y `num_heads` se refiere al número de cabezas de atención.\n",
    "- Capas densas: Las transformaciones lineales de las consultas(queries), claves(keys) y valores(values) se crean mediante `wq`, `wk` y `wv`.\n",
    "- `split_heads`: Divide el tensor de entrada en varias cabezas. El tensor resultante tendrá la forma `(batch_size, num_heads, seq_len, depth)`.\n",
    "- `call`: Este método realiza la operación de atención propiamente dicha. Primero calcula las consultas(queries), claves(keys) y valores(values) aplicando las capas densas correspondientes, las divide en cabezas y, a continuación, calcula la atención `scaled_attention`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Clase 2: Multi-Head Attention\n",
    "# -----------------------------------------------------------------------------\n",
    "# El transformer divide la atención en varios \"cabezas\" (heads).\n",
    "# Se hace una proyección lineal de Q, K y V para cada cabeza, se aplica\n",
    "# atención (ScaledDotProductAttention) de forma independiente, y luego\n",
    "# se concatena el resultado para devolverlo a la dimensión original.\n",
    "# -----------------------------------------------------------------------------\n",
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Verificamos que d_model sea divisible por num_heads\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        # Capas densas para proyectar Q, K y V\n",
    "        self.wq = layers.Dense(d_model)\n",
    "        self.wk = layers.Dense(d_model)\n",
    "        self.wv = layers.Dense(d_model)\n",
    "\n",
    "        # Capa final para la proyección tras la concatenación\n",
    "        self.dense = layers.Dense(d_model)\n",
    "        \n",
    "        # Clase para la atención con producto punto escalado\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Divide la última dimensión en (num_heads, depth).\n",
    "        Reordena la salida a (batch_size, num_heads, seq_len, depth).\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])  # para que num_heads quede en la segunda posición\n",
    "\n",
    "    def call(self, v, k, q, mask=None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        # Proyectamos Q, K y V\n",
    "        q = self.wq(q)  # (batch_size, seq_len_q, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len_k, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len_v, d_model)\n",
    "\n",
    "        # Dividimos cada una en num_heads\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # Aplicamos atención escalada\n",
    "        scaled_attention, attention_weights = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        # Reordenamos de vuelta y concatenamos\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        # Proyección final\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CAPA FEED-FORWARD    \n",
    "\n",
    "La capa feed-forward posicional se utiliza para procesar cada posición de forma independiente:   \n",
    "\n",
    "- `PositionwiseFeedforward`: Esta clase aplica dos capas densas a cada posición de forma independiente. La primera capa transforma la entrada a una dimensión superior y la segunda la reduce de nuevo al tamaño original de `d_model`.\n",
    "- `call`: Aplica las capas feed-forward secuencialmente a la entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Clase 3: Feed Forward (Positionalwise Feed-Forward Network)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Cada \"bloque\" del transformer tiene una red densa de dos capas.\n",
    "# La primera expande la dimensionalidad (ff_dim) y la segunda la reduce de nuevo.\n",
    "# -----------------------------------------------------------------------------\n",
    "class PositionwiseFeedForward(layers.Layer):\n",
    "    def __init__(self, d_model, ff_dim, **kwargs):\n",
    "        super(PositionwiseFeedForward, self).__init__(**kwargs)\n",
    "        self.dense1 = layers.Dense(ff_dim, activation='relu')\n",
    "        self.dense2 = layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. POSITIONAL ENCODING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **codificación posicional** se añade a los *embeddings* de entrada para proporcionar información sobre la posición de los tokens en la secuencia. A diferencia de las RNN y las LSTM, los transformers no captan de forma inherente la naturaleza secuencial de los datos, por lo que las codificaciones posicionales son esenciales para inyectar esta información.   \n",
    "\n",
    "- *Codificación posicional*: Esta función crea una codificación única para cada posición de la secuencia, que se añade a los embeddings de token.\n",
    "- *Seno* y *coseno*: Las posiciones se codifican utilizando funciones seno y coseno con diferentes frecuencias para distinguir las posiciones.\n",
    "\n",
    "A continuación se muestra la función para calcular las codificaciones posicionales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Clase 4: Positional Encoding\n",
    "# -----------------------------------------------------------------------------\n",
    "# El transformer no usa convoluciones ni recurencia, así que para\n",
    "# que la red \"entienda\" la posición de cada token, se inyecta información\n",
    "# posicional mediante senos y cosenos.\n",
    "# -----------------------------------------------------------------------------\n",
    "def positional_encoding(position, d_model):\n",
    "    \"\"\"\n",
    "    Genera un tensor de tamaño (position, d_model) con el encoding posicional.\n",
    "    \"\"\"\n",
    "    # Generamos las posiciones (0, 1, 2, ..., position-1)\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)\n",
    "\n",
    "    # Se aplican seno a las posiciones pares de la dimensión de modelo\n",
    "    # y coseno a las posiciones impares\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "    cosines = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = np.zeros(angle_rads.shape)\n",
    "    pos_encoding[:, 0::2] = sines\n",
    "    pos_encoding[:, 1::2] = cosines\n",
    "\n",
    "    # Devolvemos pos_encoding con una dimensión extra para batch\n",
    "    return tf.cast(pos_encoding[np.newaxis, ...], dtype=tf.float32)\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    \"\"\"\n",
    "    Función auxiliar para calcular los ángulos para el encoding posicional.\n",
    "    \"\"\"\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CAPA DE ENCODER   \n",
    "\n",
    "El codificador esta formado por varias capas codificadoras. Convierte la secuencia de entrada en un conjunto de 'embeddings' enriquecidos con información posicional.   \n",
    "\n",
    "- `Encoder`: El codificador se compone de una capa de 'embedding', una de codificación posicional, una de dropout y múltiples bloques `transformer`. Procesa la secuencia de entrada y genera una representación de la secuencia.\n",
    "- `call`: La secuencia de entrada pasa por la capa de 'embedding', se añade la codificación posicional y, a continuación, atraviesa secuencialmente los bloques `transformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Clase 5: Capa de Encoder\n",
    "# -----------------------------------------------------------------------------\n",
    "# Cada capa del encoder consiste en:\n",
    "#   1. Multi-head attention (con residual connection + layer normalization).\n",
    "#   2. Feed Forward (de dos capas, con residual connection + layer normalization).\n",
    "# -----------------------------------------------------------------------------\n",
    "class EncoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, dropout_rate=0.1, **kwargs):\n",
    "        super(EncoderLayer, self).__init__(**kwargs)\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, ff_dim)\n",
    "\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, mask=None, training=False):\n",
    "        # Multi-head attention\n",
    "        attn_output, _ = self.mha(x, x, x, mask=mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        # Feed Forward\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. CLASE ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Clase 6: Encoder\n",
    "# -----------------------------------------------------------------------------\n",
    "# El encoder está compuesto por:\n",
    "#   - Embedding + Positional Encoding\n",
    "#   - N capas de EncoderLayer\n",
    "# -----------------------------------------------------------------------------\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, ff_dim,\n",
    "                 input_vocab_size, maximum_position_encoding, dropout_rate=0.1, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Capa de embedding (para tokens)\n",
    "        self.embedding = layers.Embedding(input_vocab_size, d_model)\n",
    "\n",
    "        # Cálculo de la codificación posicional\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model, num_heads, ff_dim, dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, mask=None, training=False):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # Sumar embedding y codificación posicional\n",
    "        x = self.embedding(x)  # (batch_size, seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, mask=mask, training=training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. CAPA DE DECODER   \n",
    "\n",
    "El descodificador genera la secuencia de salida a partir de la representación codificada utilizando mecanismos para atender tanto a la salida del codificador como a los tokens generados previamente.   \n",
    "\n",
    "- `call`:La secuencia de entrada pasa por el 'embedding' y la codificación posicional y, a continuación, por los bloques `transformer` del descodificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Clase 7: Capa de Decoder\n",
    "# -----------------------------------------------------------------------------\n",
    "# Cada capa del decoder consiste en:\n",
    "#   1. Masked multi-head attention (para que el decoder no vea \"futuro\").\n",
    "#   2. Multi-head attention recibiendo la salida del encoder.\n",
    "#   3. Feed Forward.\n",
    "# Cada bloque con sus conexiones residuales y normalización de capa.\n",
    "# -----------------------------------------------------------------------------\n",
    "class DecoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, dropout_rate=0.1, **kwargs):\n",
    "        super(DecoderLayer, self).__init__(**kwargs)\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)  # masked MHA\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)  # MHA con salida del encoder\n",
    "\n",
    "        self.ffn = PositionwiseFeedForward(d_model, ff_dim)\n",
    "\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "        self.dropout3 = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, enc_output, look_ahead_mask=None, padding_mask=None, training=False):\n",
    "        # 1) Masked multi-head attention\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, mask=look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(x + attn1)\n",
    "\n",
    "        # 2) Multi-head attention con la salida del encoder\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, mask=padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(out1 + attn2)\n",
    "\n",
    "        # 3) Feed Forward\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(out2 + ffn_output)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. CLASE DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Clase 8: Decoder\n",
    "# -----------------------------------------------------------------------------\n",
    "# El decoder está compuesto por:\n",
    "#   - Embedding + Positional Encoding\n",
    "#   - N capas de DecoderLayer\n",
    "# -----------------------------------------------------------------------------\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, ff_dim,\n",
    "                 target_vocab_size, maximum_position_encoding, dropout_rate=0.1, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Embedding para la secuencia de salida\n",
    "        self.embedding = layers.Embedding(target_vocab_size, d_model)\n",
    "\n",
    "        # Codificación posicional\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [\n",
    "            DecoderLayer(d_model, num_heads, ff_dim, dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, enc_output, look_ahead_mask=None, padding_mask=None, training=False):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # Sumar embedding y codificación posicional\n",
    "        x = self.embedding(x)  # (batch_size, seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        attention_weights = {}\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x,\n",
    "                                                   enc_output,\n",
    "                                                   look_ahead_mask=look_ahead_mask,\n",
    "                                                   padding_mask=padding_mask,\n",
    "                                                   training=training)\n",
    "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "\n",
    "        return x, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. TRANSFORMER COMPLETO (ENCODER+DECODER)   \n",
    "\n",
    "El modelo final combina el codificador y el descodificador y genera las predicciones finales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Clase 9: Transformer completo (Encoder + Decoder)\n",
    "# -----------------------------------------------------------------------------\n",
    "class Transformer(keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, ff_dim,\n",
    "                 input_vocab_size, target_vocab_size, \n",
    "                 pe_input, pe_target, dropout_rate=0.1, **kwargs):\n",
    "        super(Transformer, self).__init__(**kwargs)\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, ff_dim,\n",
    "                               input_vocab_size, pe_input, dropout_rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, ff_dim,\n",
    "                               target_vocab_size, pe_target, dropout_rate)\n",
    "\n",
    "        self.final_layer = layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs, training=False, enc_padding_mask=None,\n",
    "             look_ahead_mask=None, dec_padding_mask=None):\n",
    "        \"\"\"\n",
    "        inputs: tupla (inp, tar)\n",
    "            inp -> secuencia de entrada (batch, seq_len_in)\n",
    "            tar -> secuencia de salida (batch, seq_len_out)\n",
    "        \"\"\"\n",
    "\n",
    "        inp, tar = inputs\n",
    "        # Salida del encoder\n",
    "        enc_output = self.encoder(inp, mask=enc_padding_mask, training=training)\n",
    "\n",
    "        # Salida del decoder\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output,\n",
    "            look_ahead_mask=look_ahead_mask,\n",
    "            padding_mask=dec_padding_mask,\n",
    "            training=training\n",
    "        )\n",
    "        \n",
    "        # Guardamos los pesos de atención en un atributo, \n",
    "        # pero no los retornamos como salida \"oficial\" del modelo\n",
    "        self._attention_weights = attention_weights\n",
    "\n",
    "        # Capa final densa para predecir el siguiente token\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, seq_len_out, vocab_size)\n",
    "\n",
    "        return final_output\n",
    "    \n",
    "    def get_attention_weights(self):\n",
    "        \"\"\"Método auxiliar para acceder a los pesos de atención si se desea.\"\"\"\n",
    "        return self._attention_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. EJEMPLO DE EJECUCIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EJEMPLO DE USO\n",
    "# -----------------------------------------------------------------------------\n",
    "# A continuación construimos un pequeño Transformer para mostrar cómo se usa.\n",
    "# No se entrena con datos reales, simplemente ilustra la compilación y llamada.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parámetros\n",
    "    num_layers = 2       # número de capas en encoder/decoder\n",
    "    d_model = 128        # dimensión de embeddings\n",
    "    num_heads = 4        # número de cabezas en Multi-Head Attention\n",
    "    ff_dim = 512         # dimensión interna de la red feed-forward\n",
    "    input_vocab_size = 8500\n",
    "    target_vocab_size = 8000\n",
    "    dropout_rate = 0.1\n",
    "\n",
    "    # Creamos un Transformer\n",
    "    transformer = Transformer(\n",
    "        num_layers=num_layers,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        ff_dim=ff_dim,\n",
    "        input_vocab_size=input_vocab_size,\n",
    "        target_vocab_size=target_vocab_size,\n",
    "        pe_input=10000,   # máximo de posiciones en la entrada\n",
    "        pe_target=6000,   # máximo de posiciones en la salida\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "\n",
    "    # Compilamos el modelo con optimizador y pérdida\n",
    "    # (Por ejemplo, la entropía cruzada categórica si se tratara de un problema de clasificación de tokens)\n",
    "    transformer.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Generamos datos de ejemplo (batch_size=2, secuencia de 5 tokens)\n",
    "    dummy_inp = tf.constant([[1,2,3,4,5],[4,5,6,7,0]])\n",
    "    dummy_tar = tf.constant([[1,2,3,4,5],[2,3,4,5,6]])\n",
    "\n",
    "    # Llamamos al modelo\n",
    "    # normalmentte deberíamos construir máscaras, pero para un ejemplo sencillo las omitimos\n",
    "    pred, attn = transformer((dummy_inp, dummy_tar), training=False)\n",
    "\n",
    "    print(\"Forma de la salida:\", pred.shape)  # (batch_size, seq_len_out, target_vocab_size)\n",
    "    # Por ejemplo -> (2, 5, 8000)\n",
    "\n",
    "    # Probamos un paso de entrenamiento \"dummy\"\n",
    "    # Para entrenamiento real, se necesitan datos reales y máscaras adecuadas\n",
    "    history = transformer.fit(x=(dummy_inp, dummy_tar),\n",
    "                              y=dummy_tar,  # normalmente: y son los mismos tar desplazados\n",
    "                              epochs=1,\n",
    "                              verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***LOGITS***\n",
    "Los *logits* son las salidas sin normalizar de la red. Se suelen usar en combinación con la función de pérdida de entropía cruzada, que internamente aplica la operación necesaria (softmax + cálculo de entropía cruzada) de la forma más eficiente y estable.   \n",
    "Cuando una red neuronal realiza una clasificación (imaginemos que queremos clasificar entre varias clases), la última capa suele producir un vector de dimensión igual al número de clases. A este vector se le llaman “logits”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULTADO DE EJECUCIÓN DEL TRANSFORMER\n",
    "**Forma de la salida: (5, 8000)**:\n",
    "\n",
    "El modelo está produciendo un tensor cuyas dimensiones son (sequence_length, vocab_size).\n",
    "En este caso, la secuencia de salida tiene 5 posiciones (tokens) y el vocabulario tiene 8000 posibles tokens diferentes.\n",
    "\n",
    "Si se dispone de un batch size de 1 (una sola secuencia de entrada/salida en este ejemplo), a veces TensorFlow \"colapsa\" la dimensión de batch y acaba mostrándo solo (5, 8000).\n",
    "Si se tiene un batch_size mayor a 1, típicamente se vería algo como (batch_size, seq_length, vocab_size).\n",
    "\n",
    "Indica que se está ejecutando una sola iteración de entrenamiento (un minibatch) durante la epoch (puesto que solo se ha proporcionado un conjunto de datos muy pequeño, de tamaño 1).\n",
    "\n",
    "\n",
    "**La precisión (accuracy) es 0.0.**\n",
    "\n",
    "Esto es normal en un solo paso de entrenamiento con datos aleatorios y pesos aleatorios. Si el modelo no acierta ninguno de los tokens, la exactitud para ese batch es 0.\n",
    "\n",
    "**loss: 9.0551:**\n",
    "\n",
    "Es el valor de la función de pérdida (en este caso, entropía cruzada).\n",
    "Un valor alrededor de 9 es típico cuando se genera una secuencia de logits muy dispersos sobre 8000 posibles tokens, sin haber realizado un entrenamiento real.\n",
    "\n",
    "En resumen, el mensaje de salida está indicando:\n",
    "\n",
    "- Qué forma tiene la salida de el Transformer (la capa final produce logits de dimensión [seq_len, vocab_size] o [batch_size, seq_len, vocab_size], si el batch_size no se colapsa).\n",
    "\n",
    "- Qué ocurrió en ese único paso de entrenamiento de ejemplo: la exactitud fue 0 y la pérdida aproximadamente 9.05, algo esperado al no haber entrenado con datos reales ni por varias épocas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
